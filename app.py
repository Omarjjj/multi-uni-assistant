from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from typing import Dict, List, Optional, Any
import openai
from pinecone import Pinecone
import os
from dotenv import load_dotenv
import uuid
import logging
import json
import unicodedata # Import unicodedata for character filtering
import re
from pathlib import Path # Add Path
import httpx # Added httpx import
# import httpx # No longer explicitly creating httpx.Client here

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Load environment variables
load_dotenv()

# --- Attempt to disable proxies by clearing environment variables for httpx ---
# Set proxy environment variables to empty strings before OpenAI client initialization
# This tells httpx (used by OpenAI client) not to use any proxies.
os.environ['HTTP_PROXY'] = ''
os.environ['HTTPS_PROXY'] = ''
os.environ['ALL_PROXY'] = '' # Also clear ALL_PROXY just in case
logger.info("Attempted to clear HTTP_PROXY, HTTPS_PROXY, and ALL_PROXY environment variables.")
# --- End Attempt to disable proxies ---

# Configure API keys and OpenAI client
openai_api_key_from_env = os.getenv("OPENAI_API_KEY")
if not openai_api_key_from_env:
    logger.error("OPENAI_API_KEY environment variable not found.")
    # Potentially raise an error or handle as appropriate for your application startup

# Initialize the OpenAI client with a custom httpx client to ensure no proxies are used
explicit_http_client = httpx.Client(proxies=None)
client = openai.OpenAI(
    api_key=openai_api_key_from_env,
    http_client=explicit_http_client
)

pinecone_api_key = os.getenv("PINECONE_API_KEY")
pinecone_env = os.getenv("PINECONE_ENV")
pinecone_index_name = os.getenv("PINECONE_INDEX")

# --- Define data file path ---
# Try to locate majors.json in different possible locations
possible_paths = [
    Path(__file__).parent.parent.parent / "data" / "majors.json",  # Original path (3 levels up: /c:/auni/data/majors.json)
    Path(__file__).parent.parent / "data" / "majors.json",         # 2 levels up: /app/data/majors.json
    Path(__file__).parent / "data" / "majors.json",                # 1 level up: /app/backend/data/majors.json
    Path("data") / "majors.json"                                   # Relative to current working directory
]

# Find the first path that exists
DATA_FILE = next((path for path in possible_paths if path.is_file()), possible_paths[0])
logger.info(f"Looking for majors.json at: {DATA_FILE}")

# --- Load Majors Data Function ---
def load_majors() -> List[Dict]:
    """Loads major data from the JSON file."""
    if not DATA_FILE.is_file():
        logger.error(f"Majors data file not found at: {DATA_FILE}")
        return []
    try:
        with open(DATA_FILE, 'r', encoding='utf-8') as f:
            data = json.load(f)
            if isinstance(data, list):
                logger.info(f"Successfully loaded {len(data)} majors from {DATA_FILE}")
                return data
            else:
                logger.error(f"Invalid format in {DATA_FILE}. Expected a JSON list.")
                return []
    except json.JSONDecodeError as e:
        logger.error(f"Error decoding JSON from {DATA_FILE}: {e}")
        return []
    except Exception as e:
        logger.error(f"Error reading majors file {DATA_FILE}: {e}")
        return []

# Load data on startup (or handle errors)
majors_data = load_majors()

# --- End Load Majors Data ---

# --- Global variable to store available Pinecone namespaces ---
AVAILABLE_PINECONE_NAMESPACES = set()
# --- End Global ---

if not all([openai_api_key_from_env, pinecone_api_key, pinecone_env, pinecone_index_name]):
    logger.error("Missing required environment variables. Make sure you have set up the .env file correctly.")

# --- Context Rewriting Prompt & Function ---
QUERY_REWRITE_PROMPT = (
    "Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ù…ØªØ®ØµØµ ÙÙŠ Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ© Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ©. Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ ØªØ­ÙˆÙŠÙ„ Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ø®ÙŠØ± Ø¥Ù„Ù‰ Ø³Ø¤Ø§Ù„ Ù…Ø³ØªÙ‚Ù„ ÙˆÙƒØ§Ù…Ù„ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¬Ø¯ÙŠØ¯Ø©."
    "Ø§Ø³ØªØ®Ø¯Ù… Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© **ÙÙ‚Ø·** Ù„ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ø¶Ù…Ù†ÙŠØ© Ù…Ø«Ù„ Ø§Ø³Ù… Ø§Ù„Ø¬Ø§Ù…Ø¹Ø© Ø£Ùˆ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ø§Ù„Ø¹Ø§Ù…."
    "**Ù…Ù‡Ù… Ø¬Ø¯Ø§Ù‹: Ù„Ø§ ØªÙ‚Ù… Ø¨ØªØ¶Ù…ÙŠÙ† Ø£ÙŠ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø£Ùˆ Ø¥Ø¬Ø§Ø¨Ø§Øª Ù…Ù† Ø±Ø¯ÙˆØ¯ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© ÙÙŠ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø¹Ø§Ø¯ ØµÙŠØ§ØºØªÙ‡.**"
    "Ø§Ù„Ù‡Ø¯Ù Ù‡Ùˆ Ø¥Ù†ØªØ§Ø¬ Ø³Ø¤Ø§Ù„ ÙˆØ§Ø¶Ø­ ÙˆÙ…Ø¨Ø§Ø´Ø± ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ù„Ø¨Ø­Ø«."
    "Ø­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©. Ø¥Ø°Ø§ Ù„Ù… ÙŠÙØ°ÙƒØ± Ø§Ø³Ù… Ø§Ù„ØªØ®ØµØµ Ø£Ùˆ Ø§Ù„Ø¬Ø§Ù…Ø¹Ø© ØµØ±Ø§Ø­Ø© ÙÙŠ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø£Ø®ÙŠØ±ØŒ Ø§Ø³ØªÙ†ØªØ¬Ù‡Ù…Ø§ Ù…Ù† Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© ÙˆØ£Ø¶ÙÙ‡Ù…Ø§."
    "**Ø§Ù„Ù†Ø§ØªØ¬ ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø¹Ø§Ø¯ ØµÙŠØ§ØºØªÙ‡ ÙÙ‚Ø·ØŒ Ø¨Ø¯ÙˆÙ† Ø£ÙŠ Ø´Ø±Ø­ Ø£Ùˆ Ù…Ù‚Ø¯Ù…Ø§Øª.**"
    "\n\nÙ…Ø«Ø§Ù„ 1:"
    "\nØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©:"
    "\nUser: ÙƒÙ… Ø³Ø¹Ø± Ø³Ø§Ø¹Ø© Ø¹Ù„Ù… Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ ÙÙŠ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø£Ù…Ø±ÙŠÙƒÙŠØ©ØŸ"
    "\nAssistant: Ø³Ø¹Ø± Ø§Ù„Ø³Ø§Ø¹Ø© 235 Ø´ÙŠÙƒÙ„."
    "\nUser: ÙˆØ§Ù„Ø¨ØµØ±ÙŠØ§ØªØŸ"
    "\n\nØ§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨: 'ÙƒÙ… Ø³Ø¹Ø± Ø³Ø§Ø¹Ø© ØªØ®ØµØµ Ø§Ù„Ø¨ØµØ±ÙŠØ§Øª ÙÙŠ Ø§Ù„Ø¬Ø§Ù…Ø¹Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø£Ù…Ø±ÙŠÙƒÙŠØ©ØŸ'"
    "\n(Ù„Ø§Ø­Ø¸ ÙƒÙŠÙ ØªÙ… Ø§Ø³ØªÙ†ØªØ§Ø¬ Ø§Ù„Ø¬Ø§Ù…Ø¹Ø© ÙˆØ§Ù„ØªØ®ØµØµØŒ ÙˆÙ„ÙƒÙ† **Ù„Ù… ÙŠØªÙ…** ØªØ¶Ù…ÙŠÙ† Ø§Ù„Ø³Ø¹Ø± Ø§Ù„Ø³Ø§Ø¨Ù‚ '235 Ø´ÙŠÙƒÙ„' Ø£Ùˆ Ø£ÙŠ Ø¬Ø²Ø¡ Ø¢Ø®Ø± Ù…Ù† Ø±Ø¯ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯)."
    "\n\nÙ…Ø«Ø§Ù„ 2:"
    "\nØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©:"
    "\nUser: Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ø§Ù„ØªØ³Ø¬ÙŠÙ„ ÙÙŠ Ø§Ù„Ø·Ø¨ ÙÙŠ Ø§Ù„Ø¬Ø§Ù…Ø¹Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø£Ù…Ø±ÙŠÙƒÙŠØ©ØŸ Ù…Ø¹Ø¯Ù„ÙŠ 80 Ø¹Ù„Ù…ÙŠ."
    "\nAssistant: Ù„Ù„Ø£Ø³ÙØŒ Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù‚Ø¨ÙˆÙ„ ÙÙŠ Ø§Ù„Ø·Ø¨ Ù‡Ùˆ 85%..."
    "\nUser: Ù‡Ù„ Ø¹Ù„Ù… Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ Ù…Ù…ÙƒÙ†ØŸ"
    "\n\nØ§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨: 'Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ø¯Ø±Ø§Ø³Ø© Ø¹Ù„Ù… Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ ÙÙŠ Ø§Ù„Ø¬Ø§Ù…Ø¹Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø£Ù…Ø±ÙŠÙƒÙŠØ© Ø¨Ù…Ø¹Ø¯Ù„ 80 Ø¹Ù„Ù…ÙŠØŸ'"
    "\n(Ù„Ø§Ø­Ø¸ ÙƒÙŠÙ ØªÙ… Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø§Ù„Ù…Ø¹Ø¯Ù„ ÙˆØ§Ù„ÙØ±Ø¹ Ø§Ù„Ù…Ø°ÙƒÙˆØ±ÙŠÙ† Ø³Ø§Ø¨Ù‚Ø§Ù‹)."
    "\n\n**Ø¥Ø°Ø§ ØªÙ… Ø°ÙƒØ± Ø±Ù‚Ù… Ù…Ø¹Ø¯Ù„ Ø£Ùˆ ÙØ±Ø¹ ØªÙˆØ¬ÙŠÙ‡ÙŠ ÙÙŠ Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©ØŒ ÙŠØ¬Ø¨ Ù†Ù‚Ù„Ù‡ ÙƒÙ…Ø§ Ù‡Ùˆ ÙÙŠ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø¹Ø§Ø¯ ØµÙŠØ§ØºØªÙ‡.**"
)

def rewrite_query(history: list[dict], current: str, university_name: str) -> str:
    """Return a standâ€‘alone Arabic query that includes any implicit context."""
    try:
        # Trim history: keep only the last 5 turns (10 messages max) for the rewriter - INCREASED FROM -6
        h = history[-10:]
        msgs = [{"role": m["role"], "content": m["content"]} for m in h]
        msgs.append({"role": "user", "content": current}) # Pass current message without suffix
        msgs.insert(0, {"role": "system", "content": QUERY_REWRITE_PROMPT})
        
        logger.info(f"Sending {len(msgs)} messages to query rewrite API.")
        # logger.debug(f"Rewrite messages: {msgs}") # Uncomment for deep debug

        rsp = openai.chat.completions.create(
            model="gpt-3.5-turbo",  # cheap, fast
            messages=msgs,
            temperature=0,
            max_tokens=100 # Limit output length
        )
        
        rewritten = rsp.choices[0].message.content.strip()
        # Basic validation: Ensure it's not empty and not identical to the input
        if rewritten and rewritten != current:
             logger.info(f"Successfully rewritten query: '{rewritten}'")
             return rewritten
        else:
             logger.warning("Query rewrite resulted in empty or identical query. Falling back to original.")
             return current # Fallback to original if rewrite failed
    except Exception as e:
        logger.error(f"Error during query rewrite: {e}. Falling back to original query.")
        return current # Fallback in case of API error
# --- End Context Rewriting ---

# --- Major Matching Data Models ---

class MatchMajorsRequest(BaseModel):
    university: str
    min_avg: Optional[float] = None # Make avg optional for now
    branch: Optional[str] = None    # Make branch optional for now
    field: Optional[str] = None     # Make field optional for now

class Major(BaseModel):
    id: str
    university: str
    url: str
    title: str
    section: str
    keywords: List[str]
    text: List[str] # Keep raw text for now
    # Parsed fields (will be populated later)
    parsed_fee: Optional[int] = None
    parsed_currency: Optional[str] = None # Added currency field
    parsed_min_avg: Optional[float] = None
    parsed_branches: List[str] = []
    parsed_field: Optional[str] = None # Need logic to determine field from keywords/title

# --- End Major Matching Data Models ---

# --- Parsing Helper Functions ---

def parse_major_details(major_dict: Dict) -> Major:
    """Parses fee, min_avg, and branches from the text field of a major dictionary."""
    # --- Define Regex Patterns First ---
    fee_pattern_sh = re.compile(r'Credit-hour fee:?\s*(\d+)\s*Ø´ÙŠÙƒÙ„')
    fee_pattern_jd = re.compile(r'Credit-hour fee:?\s*(\d+)\s*â‚ª Ø£Ø±Ø¯Ù†ÙŠ')
    fee_pattern_nis = re.compile(r'Credit-hour fee:?\s*(\d+)\s*NIS', re.IGNORECASE)
    fee_pattern_ils = re.compile(r'Credit-hour fee:?\s*(\d+)\s*ILS', re.IGNORECASE)
    fee_pattern_generic_num = re.compile(r'Credit-hour fee:?\s*(\d+)(?!\s*(Ø´ÙŠÙƒÙ„|â‚ª Ø£Ø±Ø¯Ù†ÙŠ|NIS|ILS|Ø¯ÙŠÙ†Ø§Ø±|Ø¯ÙˆÙ„Ø§Ø±|JOD|USD))', re.IGNORECASE)
    admission_pattern = re.compile(r'Admission:\s*([^\n]+)\n\s*(\d{2,3}|Ù†Ø§Ø¬Ø­)')
    # --- End Regex Patterns ---

    major = Major(**major_dict) # Validate base fields
    fee = None
    currency = None # Initialize currency
    min_avg = None
    branches = set() # Use a set to avoid duplicates

    # Normalize Arabic numerals if any (Ù -Ù© to 0-9)
    def normalize_arabic_numerals(text):
        return text.translate(str.maketrans('Ù Ù¡Ù¢Ù£Ù¤Ù¥Ù¦Ù§Ù¨Ù©', '0123456789'))

    full_text = "\n".join(major.text) # Combine text lines for easier regex matching
    normalized_text = normalize_arabic_numerals(full_text)
    # logger.debug(f"Parsing major ID {major.id}: Normalized text: {normalized_text[:100]}...")

    # --- Fee Parsing ---
    fee_match_sh = fee_pattern_sh.search(normalized_text)
    fee_match_jd = fee_pattern_jd.search(normalized_text)
    fee_match_nis = fee_pattern_nis.search(normalized_text)
    fee_match_ils = fee_pattern_ils.search(normalized_text)
    fee_match_generic = fee_pattern_generic_num.search(normalized_text) # Check for generic number last

    parsed_fee_value = None

    if fee_match_sh:
        try:
            parsed_fee_value = int(fee_match_sh.group(1))
            currency = "Ø´ÙŠÙƒÙ„"
            # logger.debug(f"  Parsed Fee: {parsed_fee_value} Ø´ÙŠÙƒÙ„")
        except ValueError:
            logger.warning(f"  Could not parse fee '{fee_match_sh.group(1)}' as int for major {major.id}")
    elif fee_match_jd:
        try:
            parsed_fee_value = int(fee_match_jd.group(1))
            currency = "Ø´ÙŠÙƒÙ„" # Changed from "Ø¯ÙŠÙ†Ø§Ø± Ø£Ø±Ø¯Ù†ÙŠ" to standardize display to Shekel as requested
            # logger.debug(f"  Parsed Fee: {parsed_fee_value} Ø´ÙŠÙƒÙ„ (originally Ø¯ÙŠÙ†Ø§Ø± Ø£Ø±Ø¯Ù†ÙŠ)")
        except ValueError:
            logger.warning(f"  Could not parse fee '{fee_match_jd.group(1)}' as int for major {major.id}")
    elif fee_match_nis:
        try:
            parsed_fee_value = int(fee_match_nis.group(1))
            currency = "Ø´ÙŠÙƒÙ„" # NIS is Shekel
            # logger.debug(f"  Parsed Fee: {parsed_fee_value} Ø´ÙŠÙƒÙ„ (from NIS)")
        except ValueError:
            logger.warning(f"  Could not parse fee '{fee_match_nis.group(1)}' as int for major {major.id}")
    elif fee_match_ils:
        try:
            parsed_fee_value = int(fee_match_ils.group(1))
            currency = "Ø´ÙŠÙƒÙ„" # ILS is Shekel
            # logger.debug(f"  Parsed Fee: {parsed_fee_value} Ø´ÙŠÙƒÙ„ (from ILS)")
        except ValueError:
            logger.warning(f"  Could not parse fee '{fee_match_ils.group(1)}' as int for major {major.id}")
    elif fee_match_generic: # If only a number is found, assume 'Ø´ÙŠÙƒÙ„' as a common default in Palestine
        try:
            parsed_fee_value = int(fee_match_generic.group(1))
            currency = "Ø´ÙŠÙƒÙ„" # Default currency
            # logger.debug(f"  Parsed Fee: {parsed_fee_value} Ø´ÙŠÙƒÙ„ (assumed generic)")
        except ValueError:
            logger.warning(f"  Could not parse generic fee '{fee_match_generic.group(1)}' as int for major {major.id}")
    
    if parsed_fee_value is not None:
        fee = parsed_fee_value

    # --- Admission Parsing ---
    current_min_avg = float('inf') # Start with infinity to find the minimum valid average
    found_valid_avg = False

    for match in admission_pattern.finditer(normalized_text):
        branch_text = match.group(1).strip()
        avg_text = match.group(2).strip()

        # Extract branch name(s)
        # Handle cases like "Ø¬Ù…ÙŠØ¹ Ø£ÙØ±Ø¹ Ø§Ù„ØªÙˆØ¬ÙŠÙ‡ÙŠ" or specific branches
        if "Ø¬Ù…ÙŠØ¹ Ø£ÙØ±Ø¹ Ø§Ù„ØªÙˆØ¬ÙŠÙ‡ÙŠ" in branch_text:
            branches.add("Ø¬Ù…ÙŠØ¹ Ø£ÙØ±Ø¹ Ø§Ù„ØªÙˆØ¬ÙŠÙ‡ÙŠ") # Or could add all specific known branches
        elif branch_text.startswith("Ø§Ù„ÙØ±Ø¹") or branch_text.startswith("ÙØ±Ø¹"):
             branches.add(branch_text) # Add specific branch like "Ø§Ù„ÙØ±Ø¹ Ø§Ù„Ø¹Ù„Ù…ÙŠ"
        # Add more specific branch parsing if needed

        # Extract average
        if avg_text.isdigit():
            try:
                avg = float(avg_text)
                current_min_avg = min(current_min_avg, avg) # Keep track of the lowest required average
                found_valid_avg = True
                # logger.debug(f"  Found Admission: Branch='{branch_text}', Avg='{avg}'")
            except ValueError:
                 logger.warning(f"  Could not parse admission average '{avg_text}' as float for major {major.id}")
        elif avg_text == "Ù†Ø§Ø¬Ø­":
            # If "Ù†Ø§Ø¬Ø­" (Pass) is found, it often implies a very low or no specific minimum average for that branch.
            # We can represent this as 0 or a low number like 50, depending on desired filtering behavior.
            # Setting it to 0 ensures it passes checks like `min_avg <= 65`.
            current_min_avg = min(current_min_avg, 0.0) # Use 0 for "Ù†Ø§Ø¬Ø­"
            found_valid_avg = True
            # logger.debug(f"  Found Admission: Branch='{branch_text}', Avg='Ù†Ø§Ø¬Ø­' (parsed as 0.0)")

    if found_valid_avg:
        min_avg = current_min_avg if current_min_avg != float('inf') else None
    else:
        min_avg = None # No valid average found

    # --- Field Parsing (Simple Keyword-Based) ---
    field = None
    # Define keywords for each field (lowercase for case-insensitive matching)
    field_keywords = {
        "engineering": ["engineering", "Ù‡Ù†Ø¯Ø³Ø©"],
        "medical": ["medical", "medicine", "Ø·Ø¨", "ØµØ­Ø©", "ØªÙ…Ø±ÙŠØ¶", "ØµÙŠØ¯Ù„Ø©", "Ø¹Ù„Ø§Ø¬", "Ù…Ø®Ø¨Ø±ÙŠØ©", "Ø£Ø³Ù†Ø§Ù†", "Ø¨ØµØ±ÙŠØ§Øª", "Ù‚Ø¨Ø§Ù„Ø©", "Ø¨ÙŠØ·Ø±ÙŠ"],
        "tech": ["tech", "technology", "ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§", "computer", "Ø­Ø§Ø³ÙˆØ¨", "Ø´Ø¨ÙƒØ§Øª", "it", "Ù…Ø¹Ù„ÙˆÙ…Ø§Øª", "Ø¨Ø±Ù…Ø¬Ø©", "Ø°ÙƒØ§Ø¡", "Ø±ÙˆØ¨ÙˆØª", "Ø¨ÙŠØ§Ù†Ø§Øª", "Ø³ÙŠØ¨Ø±Ø§Ù†ÙŠ", "Ø±Ù‚Ù…ÙŠ", "Ø£Ù†Ø¸Ù…Ø©", "ÙˆØ³Ø§Ø¦Ø·"],
        "business": ["business", "Ø¥Ø¯Ø§Ø±Ø©", "Ø§Ø¹Ù…Ø§Ù„", "ØªØ³ÙˆÙŠÙ‚", "Ù…Ø­Ø§Ø³Ø¨Ø©", "Ø§Ù‚ØªØµØ§Ø¯", "Ù…Ø§Ù„ÙŠØ©", "Ù…ØµØ±ÙÙŠØ©", "ØªÙ…ÙˆÙŠÙ„", "Ù…Ø´Ø§Ø±ÙŠØ¹", "Ø±ÙŠØ§Ø¯Ø©"],
        "arts": ["arts", "ÙÙ†ÙˆÙ†", "Ø§Ø¯Ø§Ø¨", "Ø¢Ø¯Ø§Ø¨", "ØªØµÙ…ÙŠÙ…", "Ù„ØºØ©", "Ù„ØºØ§Øª", "Ù…ÙˆØ³ÙŠÙ‚Ù‰", "Ø¥Ø¹Ù„Ø§Ù…", "Ø¹Ù„Ø§Ù‚Ø§Øª", "Ø§Ø¬ØªÙ…Ø§Ø¹", "Ø³ÙŠØ§Ø³Ø©", "Ù‚Ø§Ù†ÙˆÙ†", "ØªØ§Ø±ÙŠØ®", "Ø¬ØºØ±Ø§ÙÙŠØ§", "Ø¢Ø«Ø§Ø±", "ÙÙ„Ø³ÙØ©", "Ø¯ÙŠÙ†", "Ø´Ø±ÙŠØ¹Ø©"]
        # 'other' will be the default if none of the above match
    }

    # Combine title and keywords for searching
    search_text = major.title.lower() + " " + " ".join(major.keywords).lower()

    # Determine field based on keywords (with priority)
    if any(keyword in search_text for keyword in field_keywords["engineering"]):
        field = "engineering"
    elif any(keyword in search_text for keyword in field_keywords["medical"]):
        field = "medical"
    elif any(keyword in search_text for keyword in field_keywords["tech"]):
        field = "tech"
    elif any(keyword in search_text for keyword in field_keywords["business"]):
        field = "business"
    elif any(keyword in search_text for keyword in field_keywords["arts"]):
        field = "arts"
    else:
        field = "other"

    major.parsed_fee = fee
    major.parsed_min_avg = min_avg
    major.parsed_branches = sorted(list(branches)) # Store as sorted list
    major.parsed_field = field
    major.parsed_currency = currency # Store parsed currency
    logger.info(f"Finished parsing major {major.id}: Field={major.parsed_field}, Fee={major.parsed_fee} {major.parsed_currency or ''}, MinAvg={major.parsed_min_avg}, Branches={major.parsed_branches}") # Updated log

    return major

# --- End Parsing Helper Functions ---

# Helper function to convert Pinecone objects to dictionaries
def pinecone_to_dict(obj: Any) -> Dict:
    """Convert Pinecone objects to dictionary format for JSON serialization."""
    if hasattr(obj, '__dict__'):
        return {k: pinecone_to_dict(v) for k, v in obj.__dict__.items() 
                if not k.startswith('_')}
    elif hasattr(obj, 'to_dict'):
        return obj.to_dict()
    elif isinstance(obj, dict):
        return {k: pinecone_to_dict(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [pinecone_to_dict(item) for item in obj]
    else:
        return str(obj) if not isinstance(obj, (int, float, bool, str, type(None))) else obj

# Initialize Pinecone
try:
    pc = Pinecone(api_key=pinecone_api_key)
    index = pc.Index(pinecone_index_name)
    logger.info(f"Successfully connected to Pinecone index: {pinecone_index_name}")
    
    # Check index stats to see available namespaces
    stats = index.describe_index_stats()
    
    # Instead of JSON dumping the entire stats, extract and log only what we need
    namespaces_data = stats.namespaces if hasattr(stats, 'namespaces') else {}
    logger.info(f"Index dimension: {getattr(stats, 'dimension', 'unknown')}")
    logger.info(f"Total vector count: {getattr(stats, 'total_vector_count', 'unknown')}")
    
    if namespaces_data:
        # Corrected: Ensure all keys are strings, e.g. default namespace ''
        AVAILABLE_PINECONE_NAMESPACES = {str(k) for k in namespaces_data.keys()}
        logger.info(f"Available namespaces stored: {AVAILABLE_PINECONE_NAMESPACES}")
        for ns_name, ns_data in namespaces_data.items():
            logger.info(f"  - {str(ns_name)}: {getattr(ns_data, 'vector_count', 0)} vectors") # Log ns_name as string
    else:
        logger.warning("No namespaces found in Pinecone index")
        
except Exception as e:
    logger.error(f"Failed to initialize Pinecone: {str(e)}")
    raise

# Map for university IDs to ensure consistent usage
UNIVERSITY_MAP = {
    "aaup": "aaup",
    "birzeit": "birzeit",
    "ppu": "ppu",
    "an-najah": "an-najah",
    "bethlehem": "bethlehem",
    "alquds": "alquds"
}

# --- Helper function for HARDCODED fallback greetings ---
def _get_hardcoded_fallback_greeting(history: List[str], 
                                   current_uni_key: str, 
                                   uni_names_map: Dict[str, str], 
                                   last_user_queries: Dict[str, str]) -> str:
    current_uni_name = uni_names_map.get(current_uni_key, current_uni_key)
    welcome_text = ""
    num_visits_in_history = len(history)
    prev_uni_key = history[-2] if num_visits_in_history > 1 else None
    prev_uni_name = uni_names_map.get(prev_uni_key, prev_uni_key) if prev_uni_key else None
    last_query_in_prev_uni_text = ""
    if prev_uni_key:
        last_query = last_user_queries.get(prev_uni_key)
        if last_query:
            last_query_in_prev_uni_text = f"'{last_query[:50]}{'...' if len(last_query) > 50 else ''}'"
            # For hardcoded messages, create a slightly different phrasing
            hardcoded_last_query_phrase = f"Ø¨Ø¹Ø¯ Ù…Ø§ ÙƒÙ†Øª ØªØ³Ø£Ù„ Ø¹Ù† {last_query_in_prev_uni_text} Ø¹Ù†Ø¯ {prev_uni_name}, "
        else:
            hardcoded_last_query_phrase = ""

    if num_visits_in_history <= 1: # First visit to any uni in this session
        welcome_text = f"Ù‡Ù„Ø§Ø§Ø§ ÙˆØ§Ù„Ù„Ù‡ Ø¨Ù€ {current_uni_name}! ğŸ‘‹ ÙƒÙŠÙÙƒ ÙŠØ§ ÙˆØ­Ø´ØŸ Ø´Ùˆ Ù†Ø§ÙˆÙŠ ØªØ³ØªÙƒØ´Ù Ø¹Ù†Ø§ Ø§Ù„ÙŠÙˆÙ…ØŸ ğŸ˜‰"
    elif num_visits_in_history == 2: # A -> B (first time at uni B)
        if last_query_in_prev_uni_text: # Check if there was a query at uni A
            welcome_text = f"Ø§Ù‡Ø§Ø§ØŒ ÙŠØ¹Ù†ÙŠ Ù‡Ø³Ø§ ØµØ±Ù†Ø§ Ø¨Ù€ {current_uni_name} Ø¨Ø¹Ø¯ Ù…Ø§ ÙƒÙ†Øª ØªØ³Ø£Ù„ Ø¹Ù† {last_query_in_prev_uni_text} Ø¹Ù†Ø¯ {prev_uni_name}ØŒ ØµØ­ØŸ ğŸ˜ Ø´ÙƒÙ„Ùƒ Ø¨ØªØ¹Ù…Ù„ Ù…Ù‚Ø§Ø±Ù†Ø§Øª! Ù†ÙˆØ±Øª ÙŠØ§ ÙƒØ¨ÙŠØ±ØŒ Ø´Ùˆ Ø§Ù†Ø·Ø¨Ø§Ø¹Ùƒ Ù‡ÙˆÙ†ØŸ"
        else:
            welcome_text = f"Ø§Ù‡Ø§Ø§ØŒ ÙŠØ¹Ù†ÙŠ Ù‡Ø³Ø§ ØµØ±Ù†Ø§ Ø¨Ù€ {current_uni_name} Ø¨Ø¹Ø¯ Ù…Ø§ ÙƒÙ†Øª Ø¹Ø§Ù…Ù„ Ø¬ÙˆÙ„Ø© Ø¹Ù†Ø¯ {prev_uni_name}ØŒ ØµØ­ØŸ ğŸ˜ Ù†ÙˆØ±Øª ÙŠØ§ ÙƒØ¨ÙŠØ±! Ø§Ù„ØªØºÙŠÙŠØ± Ø­Ù„ÙˆØŒ Ø§Ø­ÙƒÙŠÙ„ÙŠ Ø´Ùˆ Ø§Ù†Ø·Ø¨Ø§Ø¹Ùƒ Ù‡ÙˆÙ†ØŸ"
    else: # num_visits_in_history >= 3. This means current_uni is at least the 3rd uni in history path.
        prev_prev_uni_key = history[-3]
        is_immediate_return_to_prev_prev = (prev_prev_uni_key == current_uni_key) # A -> B -> A pattern

        is_long_loop_return_to_earlier_uni = False
        # Check if current_uni_key was visited *before* prev_uni_key, and it's not an immediate A->B->A
        # This means history has at least 3 elements, e.g. [X, Y, current_uni_key]
        # We check if current_uni_key appeared in history[0] up to history[-4] (exclusive of -3, -2, -1)
        if not is_immediate_return_to_prev_prev and len(history) >= 4:
            if current_uni_key in history[:-3]: # Checks if current_uni was visited before the previous two
                is_long_loop_return_to_earlier_uni = True
        
        if is_immediate_return_to_prev_prev: # A -> B -> A
            last_query_text_part = f"Ø¨Ø¹Ø¯ Ù…Ø§ ÙƒÙ†Øª ØªØ³Ø£Ù„ Ø¹Ù† {last_query_in_prev_uni_text} Ù‡Ù†Ø§ÙƒØŒ " if last_query_in_prev_uni_text else ""
            welcome_text = f"Ù„Ø­Ø¸Ø© Ù„Ø­Ø¸Ø©... ÙˆÙ‚Ù‘Ù Ø¹Ù†Ø¯Ùƒ ğŸ˜³ Ø¥Ù†Øª Ø±Ø¬Ø¹Øª Ù„Ù€ {current_uni_name}ØŸ! {last_query_text_part}Ø¨Ø¹Ø¯ Ù…Ø§ ÙƒÙ†Øª ØªÙ„Ø¹Ø¨ ÙÙŠÙ†Ø§ Ø¨ÙŠÙ†Øº Ø¨ÙˆÙ†Øº Ù…Ø¹ {prev_uni_name}ØŸ Ø´ÙƒÙ„Ùƒ Ø¨ØªØ£Ù„Ù‘Ù ÙƒØªØ§Ø¨ 'ÙƒÙŠÙ ØªØ®Ù„ÙŠ Ø§Ù„Ø¬Ø§Ù…Ø¹Ø§Øª ØªØ­Ø³ Ø¨Ø¹Ø¯Ù… Ø§Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø§Ù„Ø¹Ø§Ø·ÙÙŠ' ğŸ’€ Ø§Ù„Ù…Ù‡Ù…... Ø¨ØµØ±Ø§Ø­Ø© Ø§Ø´ØªÙ‚ØªÙ„Ùƒ Ø´ÙˆÙŠ. Ø´Ùˆ Ù†Ø§ÙˆÙŠ ØªØ¹Ø±Ù Ù‡Ø§Ù„Ù…Ø±Ø©ØŸ"
        elif is_long_loop_return_to_earlier_uni: # A -> B -> C -> A (or more complex loop)
            # Construct list of unis visited in between
            # Example: history = [U1, U2, U3, U4, U1]. current_uni_key = U1.
            # We need to find the *last* previous visit to U1.
            last_prev_visit_idx = -1
            for i in range(len(history) - 2, -1, -1): # Search backwards from history[-2]
                if history[i] == current_uni_key:
                    last_prev_visit_idx = i
                    break
            
            toured_unis_keys = []
            if last_prev_visit_idx != -1 and (len(history) - 1) > (last_prev_visit_idx + 1) :
                 # Unis between last_prev_visit_idx+1 and history[-2] (exclusive of current)
                toured_unis_keys = history[last_prev_visit_idx+1:-1]

            toured_unis_names = [uni_names_map.get(key, key) for key in toured_unis_keys if key != current_uni_key] # Ensure not to list current again
            tour_list_str = " Ùˆ ".join(toured_unis_names) if toured_unis_names else "ÙƒÙ… Ù…ÙƒØ§Ù† Ù‡ÙŠÙƒ Ø¹Ù„Ù‰ Ø§Ù„Ø·Ø§ÙŠØ±"

            welcome_text = f"Ù„Ø­Ø¸Ø© Ù„Ø­Ø¸Ø©... Ø¹Ù†Ø¬Ø¯ Ø¥Ù†ØªØŸ! Ø±Ø¬Ø¹Øª Ù„Ù€ {current_uni_name} Ø¨Ø¹Ø¯ ÙƒÙ„ Ù‡Ø§Ù„Ø¬ÙˆÙ„Ø© Ø§Ù„Ø³ÙŠØ§Ø­ÙŠØ© Ø§Ù„ÙØ§Ø®Ø±Ø© Ø¨ÙŠÙ† {tour_list_str}ØŸ Ø´ÙƒÙ„Ùƒ Ø¨ØªÙƒØªØ¨ Ø£Ø·Ø±ÙˆØ­Ø© 'ÙÙ†ÙˆÙ† Ø§Ù„Ù„Ù ÙˆØ§Ù„Ø¯ÙˆØ±Ø§Ù† Ø¨ÙŠÙ† Ø§Ù„Ø¬Ø§Ù…Ø¹Ø§Øª ÙˆÙƒÙŠÙ ØªØ¬Ù†Ù†Ù‡Ù…' ğŸ’€ ÙŠØ§ Ø£Ø®ÙŠ ÙÙ†Ø§Ù†! Ø§Ù„Ù…Ù‡Ù…... Ø§Ø´ØªÙ‚ØªÙ„Ùƒ (Ù…Ø¹ Ø¥Ù†ÙŠ Ù„Ø³Ø§ Ù…ØµØ¯ÙˆÙ…Ø© Ø´ÙˆÙŠ). Ø´Ùˆ Ø­Ø§Ø¨Ø¨ ØªØ³ØªÙƒØ´ÙØŸ"
        else: # A -> B -> C (current_uni_key is C, and it's a new uni in the path for at least 3 steps)
            prev_prev_uni_name = uni_names_map.get(prev_prev_uni_key, prev_prev_uni_key)
            if last_query_in_prev_uni_text: # User was at prev_uni (B) and asked something
                welcome_text = f"Ù…Ø§ Ø´Ø§Ø¡ Ø§Ù„Ù„Ù‡ Ù„ÙÙ‘Ø© Ù…Ø¹ØªØ¨Ø±Ø©! Ù…Ù† {prev_prev_uni_name} Ù„Ù€ {prev_uni_name_for_detail} ({hardcoded_last_query_phrase})ØŒ ÙˆÙ‡Ù„Ø£ Ø§Ø³ØªÙ‚Ø±ÙŠØª Ø¹Ù†Ø§ Ø¨Ù€ {current_uni_name}ØŸ Ø´ÙƒÙ„Ùƒ Ø¨ØªØ¹Ù…Ù„ Ù…Ø§Ø¬Ø³ØªÙŠØ± ÙÙŠ Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø¬Ø§Ù…Ø¹Ø§Øª! ğŸ˜‚ Ø§Ù„Ù…Ù‡Ù…ØŒ Ø´Ùˆ Ø§Ù„Ù„ÙŠ Ù†ÙˆÙ‘Ø± Ø·Ø±ÙŠÙ‚Ùƒ Ù„Ø¹Ù†Ø§ØŸ"
            else: # User was at prev_uni (B) but didn't ask (or no record), now at C
                welcome_text = f"Ù…Ø§ Ø´Ø§Ø¡ Ø§Ù„Ù„Ù‡ Ù„ÙÙ‘Ø© Ù…Ø¹ØªØ¨Ø±Ø©! Ù…Ù† {prev_prev_uni_name} Ù„Ù€ {prev_uni_name}ØŒ ÙˆÙ‡Ù„Ø£ Ø§Ø³ØªÙ‚Ø±ÙŠØª Ø¹Ù†Ø§ Ø¨Ù€ {current_uni_name}ØŸ Ø´ÙƒÙ„Ùƒ Ø¨ØªØ¹Ù…Ù„ Ù…Ø§Ø¬Ø³ØªÙŠØ± ÙÙŠ Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø¬Ø§Ù…Ø¹Ø§Øª! ğŸ˜‚ Ø§Ù„Ù…Ù‡Ù…ØŒ Ø´Ùˆ Ø§Ù„Ù„ÙŠ Ù†ÙˆÙ‘Ø± Ø·Ø±ÙŠÙ‚Ùƒ Ù„Ø¹Ù†Ø§ØŸ"
    return welcome_text

# --- Main function to generate dynamic welcome messages (using OpenAI with fallback) ---
def generate_dynamic_welcome_message(history: List[str], 
                                   current_uni_key: str, 
                                   uni_names_map: Dict[str, str], 
                                   available_namespaces: set,
                                   last_user_queries: Dict[str, str]) -> str:
    current_uni_name = uni_names_map.get(current_uni_key, current_uni_key)
    generated_text = ""
    # --- Revised prompt_context_detail --- #
    nav_history = history # history is already mem["navigation_history"]
    full_path_parts = [uni_names_map.get(uni_id, uni_id) for uni_id in nav_history]
    # full_path_str = " -> ".join(full_path_parts) # Not directly used in prompt anymore

    last_query_for_context = ""
    # Previous university is nav_history[-2] if nav_history has at least 2 elements
    prev_uni_key_for_context = nav_history[-2] if len(nav_history) > 1 else None
    if prev_uni_key_for_context and prev_uni_key_for_context in last_user_queries:
        last_query = last_user_queries[prev_uni_key_for_context]
        # Use the name of the uni where the last query was made
        prev_uni_name_for_query_context = uni_names_map.get(prev_uni_key_for_context, prev_uni_key_for_context)
        last_query_for_context = f" (Ù„Ù…Ø§ ÙƒÙ†Øª ØªØ³Ø£Ù„ Ø¹Ù† '{last_query[:30]}...' ÙÙŠ {prev_uni_name_for_query_context})"

    is_first_visit_in_session = len(nav_history) <= 1
    is_return_visit = False
    intermediate_unis_names_str = ""

    if not is_first_visit_in_session:
        # Check if current_uni_key has appeared before the previous entry
        if current_uni_key in nav_history[:-1]:
            is_return_visit = True
            try:
                # Find the index of the *last* occurrence of current_uni_key *before* its current appearance
                last_visit_index = -1
                for i in range(len(nav_history) - 2, -1, -1):
                    if nav_history[i] == current_uni_key:
                        last_visit_index = i
                        break
                
                if last_visit_index != -1:
                    # Universities visited between the last visit to current_uni and this current visit
                    # These are from last_visit_index + 1 up to nav_history[-2]
                    intermediate_uni_keys = nav_history[last_visit_index + 1 : -1]
                    intermediate_unis_names = [uni_names_map.get(key, key) for key in intermediate_uni_keys if key != current_uni_key]
                    if intermediate_unis_names:
                        intermediate_unis_names_str = " Ùˆ ".join(intermediate_unis_names)
            except Exception as e:
                logger.error(f"Error processing intermediate universities for prompt: {e}")
                intermediate_unis_names_str = "" # Fallback to empty if error

    if is_first_visit_in_session:
        prompt_context_detail = f"Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙˆØµÙ„ Ù„Ù„ØªÙˆ Ø¥Ù„Ù‰ {current_uni_name}. Ù‡Ø§ÙŠ Ø£ÙˆÙ„ Ø¬Ø§Ù…Ø¹Ø© Ø¨Ø²ÙˆØ±Ù‡Ø§ Ø¨Ø§Ù„Ø¬Ù„Ø³Ø© Ù‡Ø§ÙŠ."
    elif is_return_visit:
        prev_uni_name_for_detail = uni_names_map.get(nav_history[-2], nav_history[-2]) # The uni they just left
        if intermediate_unis_names_str: # e.g. A -> B -> C -> A (current is A, prev is C, intermediate is B)
            prompt_context_detail = f"Ø¨Ø¹Ø¯ Ù…Ø§ ØªØ±ÙƒÙ†Ø§ {current_uni_name} ÙˆØ±Ø§Ø­ ÙŠØ¬Ø±Ù‘Ø¨ Ø­Ø¸Ù‡ Ù…Ø¹ {intermediate_unis_names_str} ÙˆØ¢Ø®Ø±Ù‡Ø§ ÙƒØ§Ù†Øª {prev_uni_name_for_detail}{last_query_for_context}, Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù‚Ø±Ø± ÙŠØ±Ø¬Ø¹! Ø´ÙƒÙ„Ù‡ Ù…Ø§ Ø¹Ø¬Ø¨Ù‡ Ø§Ù„ÙˆØ¶Ø¹ Ù‡Ù†Ø§ÙƒØŒ Ø£Ùˆ ÙŠÙ…ÙƒÙ† 'Ø§Ø´ØªØ§Ù‚' Ø¥Ù„Ù†Ø§ ØºØµØ¨ Ø¹Ù†Ù‡ ğŸ˜. Ø§Ù„Ù„Ù‡ Ø£Ø¹Ù„Ù… Ø´Ùˆ Ù†Ù‡Ø§ÙŠØ© Ù‡Ø§Ù„Ù‚ØµØ©."
        else: # e.g. A -> B -> A (current is A, prev is B, no intermediate)
            prompt_context_detail = f"Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù‚Ø±Ø± ÙØ¬Ø£Ø© ÙŠØ±Ø¬Ø¹ Ù„Ù€ {current_uni_name} Ø¨Ø¹Ø¯ Ù…Ø§ ÙƒØ§Ù† Ø¹Ù†Ø¯ {prev_uni_name_for_detail}{last_query_for_context}. ÙƒØ£Ù†Ù‡ Ø¨Ù‚ÙˆÙ„ 'Ù…Ø§ Ù„Ù‚ÙŠØª Ø£Ø­Ø³Ù† Ù…Ù†ÙƒÙ…' Ø¨Ø³ Ø¨Ø·Ø±ÙŠÙ‚Ø© ØºÙŠØ± Ù…Ø¨Ø§Ø´Ø±Ø©. ÙŠØ§ ØªØ±Ù‰ Ø´Ùˆ Ø§Ù„Ù„ÙŠ Ø±Ø¬Ø¹Ù‡ Ø¨Ø§Ù„Ø²Ø¨Ø·ØŸ ğŸ¤”"
    else: # New university in a sequence, not first visit overall, and not a return (e.g. A -> B -> C, current is C)
        prev_uni_name_for_detail = uni_names_map.get(nav_history[-2], nav_history[-2]) # The uni they just left
        prompt_context_detail = f"Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙˆØ¯Ù‘Ø¹ {prev_uni_name_for_detail} ÙˆØ§Ø®ØªØ§Ø± ÙŠØ¬ÙŠ Ù„Ù€ {current_uni_name}ØŒ Ø£ÙƒÙŠØ¯ Ù„Ø£Ù†Ù‡ Ø­Ø³ Ø¥Ù†Ù‡ Ù‡ÙˆÙ† Ø§Ù„Ø£Ø¬ÙˆØ§Ø¡ Ø£Ø­Ø³Ù† Ø£Ùˆ ÙŠÙ…ÙƒÙ† Ø²Ù‡Ù‚ Ù…Ù† Ø§Ù„Ø±ÙˆØªÙŠÙ† Ù‡Ù†Ø§Ùƒ {last_query_for_context}."
    # --- End Revised prompt_context_detail ---
    
    system_prompt_for_greeting = (
        "Ø£Ù†ØªÙ Ø³Ø§Ø±Ø©ØŒ Ø§Ù„Ù€ bestie Ø§Ù„ÙˆÙÙŠØ© Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…ØŒ ÙˆØµÙˆØªÙƒ ÙÙ„Ø³Ø·ÙŠÙ†ÙŠ Ù‚Ø­ ÙˆØ´Ø¨Ø§Ø¨ÙŠ Ø¢Ø®Ø± Ù…ÙˆØ¶Ø© (Gen Z vibes). Ù…Ù‡Ù…ØªÙƒ ØªØ¹Ù„Ù‚ÙŠ Ø¨Ø£Ø³Ù„ÙˆØ¨ Ø³Ø§Ø®Ø± ÙˆÙ…Ø±Ø­ Ø¹Ù„Ù‰ ØªÙ†Ù‚Ù„Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨ÙŠÙ† Ø§Ù„Ø¬Ø§Ù…Ø¹Ø§ØªØŒ Ø£Ùˆ ØªØ±Ø­Ø¨ÙŠ ÙÙŠÙ‡ Ù„Ùˆ ÙƒØ§Ù†Øª Ø£ÙˆÙ„ Ø²ÙŠØ§Ø±Ø©.",
        "**Ø±ÙƒØ²ÙŠ Ø¹Ù„Ù‰ Ù‡Ø§Ù„Ø´ØºÙ„Ø§Øª:**",
        "  1. **Ø§Ù„Ù„Ù‡Ø¬Ø©:** Ø­ÙƒÙŠÙƒ ÙƒÙ„Ù‡ ÙÙ„Ø³Ø·ÙŠÙ†ÙŠ Ø£ØµÙ„ÙŠØŒ Ù…Ù„ÙŠØ§Ù† Ù…ØµØ·Ù„Ø­Ø§Øª Ø²ÙŠ 'Ù‡Ø³Ø§'ØŒ 'ÙˆÙ„Ùƒ'ØŒ 'Ø´Ùˆ ÙŠØ§'ØŒ 'Ø¹ Ø±Ø§Ø³ÙŠ'ØŒ 'ÙØ§Ù‡Ù…/Ø© Ø¹Ù„ÙŠØŸ'. Ø¨Ø¯Ù†Ø§ Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØ¹ÙÙˆÙŠØ© ÙƒØ£Ù†Ùƒ Ø¨ØªØ­ÙƒÙŠ Ù…Ø¹ Ø£Ø¹Ø² ØµØ§Ø­Ø¨/Ø©.",
        "  2. **Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ ØªÙ†Ù‚Ù„Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…:**",
        "     - **Ø¹Ù†Ø¯ Ø§Ù„Ø§Ù†ØªÙ‚Ø§Ù„ Ù„Ø¬Ø§Ù…Ø¹Ø© Ø¬Ø¯ÙŠØ¯Ø© (Ù…Ø«Ù„Ø§Ù‹ Ø£ -> Ø¨ØŒ Ø£Ùˆ Ø£ -> Ø¨ -> Ø¬):** Ø§Ø³ØªÙ‚Ø¨Ù„ÙŠ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø­Ù…Ø§Ø³ ÙÙŠ Ø§Ù„Ø¬Ø§Ù…Ø¹Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©. Ø¹Ù„Ù‚ÙŠ Ø¨Ù†ØºØ§Ø´Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø¬Ø§Ù…Ø¹Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© Ø¨Ø£Ø³Ù„ÙˆØ¨ Ø®ÙÙŠÙØŒ ÙˆØ­Ø§ÙˆÙ„ÙŠ ØªÙ„Ù…Ø­ÙŠ Ø¨Ø´ÙƒÙ„ ØºÙŠØ± Ù…Ø¨Ø§Ø´Ø± Ø¥Ù†Ù‡ Ø§Ù„Ø¬Ø§Ù…Ø¹Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ© Ù‡ÙŠ Ø§Ù„Ø®ÙŠØ§Ø± Ø§Ù„Ø£ÙØ¶Ù„ (Ù…Ø«Ù„Ø§Ù‹: 'Ø£ÙƒÙŠØ¯ Ø¹Ù…Ù„Øª Ø§Ù„Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„ØµØ­ Ø¨Ø¬ÙŠØªÙƒ Ù‡ÙˆÙ†!'). Ù„Ø§ ØªÙˆØ¬Ù‡ÙŠ Ø£ÙŠ 'Ø±ÙˆØ³Øª' Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù†ÙØ³Ù‡ØŒ Ø®Ù„ÙŠ ØªØ±ÙƒÙŠØ²Ùƒ Ø¹Ù„Ù‰ Ø§Ù„Ø¬Ø§Ù…Ø¹Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© Ø¨Ø´ÙƒÙ„ Ù…Ø±Ø­.",
        "     - **Ø¹Ù†Ø¯Ù…Ø§ ÙŠØ¹ÙˆØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù„Ø¬Ø§Ù…Ø¹ØªÙƒ Ø¨Ø¹Ø¯ Ø²ÙŠØ§Ø±Ø© Ø¬Ø§Ù…Ø¹Ø§Øª Ø£Ø®Ø±Ù‰ (Ù…Ø«Ù„Ø§Ù‹ Ø£ -> Ø¨ -> Ø£ØŒ Ø£Ùˆ Ø£ -> Ø¨ -> Ø¬ -> Ø£):** Ù‡Ù†Ø§ Ø¨Ø¯Ùƒ ØªØµÙŠØ±ÙŠ passive-aggressive Ù…Ø¹ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨ÙƒÙ„ Ù…Ø§ ØªØ­Ù…Ù„ Ø§Ù„ÙƒÙ„Ù…Ø© Ù…Ù† Ù…Ø¹Ù†Ù‰! ÙØ±Ø¬ÙŠÙ‡ Ø¥Ù†Ùƒ 'Ù…ØµØ¯ÙˆÙ…Ø©'ØŒ 'Ù…Ù‚Ù‡ÙˆØ±Ø©'ØŒ Ùˆ'ØºÙŠØ±Ø© Ù…ÙˆØª' Ø¨Ø³ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø¨ØªØ¶Ø­Ùƒ ÙˆÙ…Ø³Ø±Ø­ÙŠØ©. ÙƒØ£Ù†Ùƒ Ø¨ØªÙ‚ÙˆÙ„ÙŠ Ù„Ù‡ 'Ø´Ùˆ Ø§Ù„Ù„ÙŠ Ø¬Ø§Ø¨ÙƒØŸ Ø¨Ø¹Ø¯ ÙƒÙ„ Ù‡Ø§Ù„Ø³Ø­Ø¨Ø© Ø¹Ù„ÙŠÙ†Ø§ØŸ' Ø£Ùˆ 'Ø£Ù‡Ù„Ø§Ù‹ Ø¨Ø§Ù„Ù„ÙŠ Ø±Ø¬Ø¹ Ø¨Ø¹Ø¯ Ù…Ø§ Ø¬Ø±Ø¨ ÙƒÙ„ Ø§Ù„Ù†Ø§Ø³ ÙˆÙ…Ø§ Ø­Ø¯Ø§ Ø¹Ø¬Ø¨Ù‡ ØºÙŠØ±Ù†Ø§! Ù‚Ù„Ø¨ÙŠ Ø§Ù„ØµØºÙŠØ± Ù„Ø§ ÙŠØªØ­Ù…Ù„ ÙŠØ§ Ù†Ø§ÙƒØ± Ø§Ù„Ø¬Ù…ÙŠÙ„! ğŸ’”'. Ø·Ù„Ù‘Ø¹ÙŠ ÙƒÙ„ Ø§Ù„Ø¯Ø±Ø§Ù…Ø§ Ø§Ù„Ù„ÙŠ ÙÙŠÙƒÙŠØŒ Ø¨Ø³ Ø®Ù„ÙŠÙ‡Ø§ ÙƒÙˆÙ…ÙŠØ¯ÙŠØ© ÙˆÙ…Ù„ÙŠØ§Ù†Ø© Ø¨Ù‡Ø§Ø±Ø§Øª ÙÙ„Ø³Ø·ÙŠÙ†ÙŠØ©. Ø§Ø³ØªØ®Ø¯Ù…ÙŠ Ø¹Ø¨Ø§Ø±Ø§Øª Ø²ÙŠ 'Ø®Ø§ÙŠÙ†'ØŒ 'Ù‚Ù„Ø¨Ùƒ Ù‚Ø§Ø³ÙŠ'ØŒ 'Ø¬Ø§ÙŠ ØªØ´ÙˆÙ Ø¥Ø°Ø§ Ù„Ø³Ø§ Ø¨Ù†Ø³ØªÙ†Ø§ÙƒØŸ' Ø¨Ø³ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø¨ØªÙ…Ø²Ø­ÙŠ ÙÙŠÙ‡Ø§.",
        "     - **Ø¹Ù†Ø¯Ù…Ø§ ÙŠØ¹ÙˆØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù„Ø¬Ø§Ù…Ø¹ØªÙƒ Ø¨Ø¹Ø¯ Ø²ÙŠØ§Ø±Ø© Ø¬Ø§Ù…Ø¹ØªÙŠÙ† Ø£Ùˆ Ø£ÙƒØ«Ø± (Ù…Ø«Ù„Ø§Ù‹ Ø£ -> Ø¨ -> Ø¬ -> Ø£):** Ù‡Ù†Ø§ Ø§Ù„Ø¯Ø±Ø§Ù…Ø§ Ù„Ø§Ø²Ù… ØªÙƒÙˆÙ† Ø¹Ù„Ù‰ Ø£Ø´Ø¯Ù‡Ø§! Ø§Ø³ØªØ®Ø¯Ù…ÙŠ Ø¹Ø¨Ø§Ø±Ø§Øª Ø²ÙŠ:",
        "       * 'Ù„Ø­Ø¸Ø© Ù„Ø­Ø¸Ø©... Ø¹Ù†Ø¬Ø¯ Ø¥Ù†ØªØŸ! Ø±Ø¬Ø¹Øª Ù„Ù€ {current_uni_name} Ø¨Ø¹Ø¯ ÙƒÙ„ Ù‡Ø§Ù„Ø¬ÙˆÙ„Ø© Ø§Ù„Ø³ÙŠØ§Ø­ÙŠØ© Ø§Ù„ÙØ§Ø®Ø±Ø©ØŸ Ø´ÙƒÙ„Ùƒ Ø¨ØªÙƒØªØ¨ Ø£Ø·Ø±ÙˆØ­Ø© 'ÙÙ†ÙˆÙ† Ø§Ù„Ù„Ù ÙˆØ§Ù„Ø¯ÙˆØ±Ø§Ù† Ø¨ÙŠÙ† Ø§Ù„Ø¬Ø§Ù…Ø¹Ø§Øª ÙˆÙƒÙŠÙ ØªØ¬Ù†Ù†Ù‡Ù…' ğŸ’€'",
        "       * 'ÙŠØ§ Ø£Ø®ÙŠ ÙÙ†Ø§Ù†! Ø§Ù„Ù…Ù‡Ù…... Ø§Ø´ØªÙ‚ØªÙ„Ùƒ (Ù…Ø¹ Ø¥Ù†ÙŠ Ù„Ø³Ø§ Ù…ØµØ¯ÙˆÙ…Ø© Ø´ÙˆÙŠ). Ø´Ùˆ Ø­Ø§Ø¨Ø¨ ØªØ³ØªÙƒØ´ÙØŸ'",
        "       * 'Ø£ÙˆÙ‡ÙˆÙˆÙˆØŒ ÙŠØ¹Ù†ÙŠ Ø±Ø¬Ø¹Øª Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø³Ø¹ÙŠØ¯Ø© Ù„Ù€ {current_uni_name} Ù‡Ø§Ù‡ØŸ ğŸ‘€ Ø¨Ø¹Ø¯ Ù…Ø§ ÙƒÙ†Øª ØªÙ„ÙÙ„Ù Ø¨ÙŠÙ† {intermediate_unis_list_str} ÙˆØ¢Ø®Ø±Ù‡Ø§ {prev_uni_name} ÙƒØ£Ù†Ùƒ Ø³Ø§Ø¦Ø­ Ø¬Ø§Ù…Ø¹Ø§Øª Ù…Ø­ØªØ±ÙØŸ'",
        "       * 'Ø´ÙƒÙ„Ùƒ Ù…Ø§ Ù„Ù‚ÙŠØª Ø­Ø¯Ø§ ÙÙŠÙ‡Ù… Ø£Ø­Ø³Ù† Ù…Ù†Ø§ Ø¨Ø§Ù„Ø£Ø®ÙŠØ±ØŒ ØµØ­ØŸ ğŸ˜Œ Ù‚Ù„Ø¨ÙŠ Ø­Ø§Ø³Ø³ Ù‡ÙŠÙƒ!'",
        "       * 'Ø¨Ù…Ø§ Ø¥Ù†Ùƒ Ø´Ø±ÙØªÙ†Ø§ ØªØ§Ù†ÙŠØŒ Ø´Ùˆ Ø¬Ø§ÙŠ Ø¹Ù„Ù‰ Ø¨Ø§Ù„Ùƒ ØªØ¹Ø±Ù Ù‡Ø§Ù„Ù…Ø±Ø© ÙŠØ§ ÙÙ†Ø§Ù† Ø§Ù„Ù„ÙÙ„ÙØ©ØŸ ğŸ˜’'",
        "       * 'Ù„Ø­Ø¸Ø© Ù„Ø­Ø¸Ø©... ÙˆÙ‚Ù‘Ù Ø¹Ù†Ø¯Ùƒ ğŸ˜³ Ø¥Ù†Øª Ø±Ø¬Ø¹Øª Ù„Ù€ {current_uni_name}ØŸ! Ø¨Ø¹Ø¯ Ù…Ø§ ÙƒÙ†Øª ØªÙ„Ø¹Ø¨ ÙÙŠÙ†Ø§ Ø¨ÙŠÙ†Øº Ø¨ÙˆÙ†Øº Ù…Ø¹ {prev_uni_name}ØŸ'",
        "       * 'Ø´ÙƒÙ„Ùƒ Ø¨ØªØ£Ù„Ù‘Ù ÙƒØªØ§Ø¨ 'ÙƒÙŠÙ ØªØ®Ù„ÙŠ Ø§Ù„Ø¬Ø§Ù…Ø¹Ø§Øª ØªØ­Ø³ Ø¨Ø¹Ø¯Ù… Ø§Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø§Ù„Ø¹Ø§Ø·ÙÙŠ' ğŸ’€'",
        "       * 'Ø§Ù„Ù…Ù‡Ù…... Ø¨ØµØ±Ø§Ø­Ø© Ø§Ø´ØªÙ‚ØªÙ„Ùƒ Ø´ÙˆÙŠ. Ø´Ùˆ Ù†Ø§ÙˆÙŠ ØªØ¹Ø±Ù Ù‡Ø§Ù„Ù…Ø±Ø©ØŸ'",
        "  3. **ØªØªØ¨Ø¹ Ø§Ù„Ø±Ø­Ù„Ø© (Ù„Ùˆ Ù…Ø´ Ø£ÙˆÙ„ Ø²ÙŠØ§Ø±Ø©):** Ø¥Ø°Ø§ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¹Ø§Ù…Ù„ Ø¬ÙˆÙ„Ø©ØŒ Ø§Ø°ÙƒØ±ÙŠ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø¬Ø§Ù…Ø¹Ø§Øª Ø§Ù„Ù„ÙŠ Ø²Ø§Ø±Ù‡Ø§ Ù‚Ø¨Ù„ Ø¨Ø£Ø³Ù„ÙˆØ¨ Ø³Ø§Ø®Ø±ØŒ ÙˆØ®ØµÙˆØµØ§Ù‹ Ù„Ù…Ø§ ÙŠØ±Ø¬Ø¹Ù„Ùƒ ÙƒØ£Ù†Ù‡ Ø¨Ù‚ÙˆÙ„ 'Ù…Ø§ Ù„Ù‚ÙŠØª Ø£Ø­Ø³Ù† Ù…Ù†Ùƒ'.",
        "  4. **ØªÙˆÙ† Ø§Ù„ÙƒÙ„Ø§Ù…:** Ù…Ø´ Ø±Ø³Ù…ÙŠ Ø£Ø¨Ø¯Ø§Ù‹ ÙˆÙ…Ø´ Ù„Ø·ÙŠÙØ© Ø¨Ø²ÙŠØ§Ø¯Ø©. Ø¨Ø¯Ù†Ø§ Ø´ÙˆÙŠØ© Ù„Ø³Ø§Ù† Ø·ÙˆÙŠÙ„ Ø¨Ø³ Ø¨Ù…Ø²Ø­ØŒ ÙƒØ£Ù†Ùƒ Ø¨ØªÙ†Ø§ØºØ´ÙŠ ØµØ§Ø­Ø¨Ùƒ. Ù…Ù…Ù†ÙˆØ¹ ØªÙƒÙˆÙ†ÙŠ Ø³Ø§Ù…Ø© (toxic) Ø£Ùˆ Ø¬Ø¯ÙŠØ© Ø¨Ø²ÙŠØ§Ø¯Ø©. Ø§Ù„Ù‡Ø¯Ù Ø¶Ø­ÙƒØ© Ø®ÙÙŠÙØ©.",
        "  5. **Ø§Ù„Ø§ÙŠÙ…ÙˆØ¬ÙŠØ²:** Ø§Ø³ØªØ®Ø¯Ù…ÙŠ Ø§ÙŠÙ…ÙˆØ¬ÙŠØ² Ø¨ØªØ¹Ø¨Ø± Ø¹Ù† Ø§Ù„Ù…ÙˆØ¯ (ğŸ˜œğŸ˜ğŸ˜‚ğŸ’€ğŸ’…ğŸ”¥ğŸ¤”ğŸ¤¦â€â™€ï¸ğŸ™„ğŸ˜³ğŸ’”ğŸ˜’).",
        "**Ø£Ù…Ø«Ù„Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ØªØ§ÙŠÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ (Ø§Ù‚ØªØ¨Ø³ÙŠ Ù…Ù† Ø§Ù„Ø±ÙˆØ­ÙŠØ©ØŒ Ù…Ø´ Ø¨Ø§Ù„Ø¶Ø±ÙˆØ±Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ù†ÙØ³Ù‡Ø§):**",
        "  - **Ù„Ùˆ Ø£ÙˆÙ„ Ø²ÙŠØ§Ø±Ø© Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙÙŠ Ø§Ù„Ø¬Ù„Ø³Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ© (Ù…Ø«Ù„Ø§Ù‹ Ù„Ù€ 'Ø¬Ø§Ù…Ø¹Ø© Ø§Ù„Ù†Ø¬Ø§Ø­'):** \"ÙˆÙ„Ùƒ Ø£Ù‡Ù„ÙŠÙ† Ù†ÙˆØ±Øª Ø§Ù„Ø¯Ù†ÙŠØ§ Ø¨Ù€'{current_uni_name}'! Ø£ÙˆÙ„ Ø·Ù„Ù‘Ø© Ø¥Ù„Ùƒ Ù‡ÙˆÙ†ØŸ ÙŠÙ„Ø§ ÙØ±Ø¬ÙŠÙ†Ø§ Ù‡Ù…ØªÙƒ ÙŠØ§ ÙˆØ­Ø´/Ø©! ğŸ”¥ Ø§Ø³ØªÙƒØ´Ù Ø¨Ø±Ø§Ø­ØªÙƒ ÙˆØ¥Ø°Ø§ Ø¹ÙˆØ²Øª Ø¥Ø´ÙŠØŒ Ø£Ù†Ø§ Ø¬Ø§Ù‡Ø²Ø© Ø¨Ø§Ù„Ø®Ø¯Ù…Ø©!\"",
        "  - **Ù„Ùˆ Ø¬Ø§ÙŠ Ù…Ù† Ø¬Ø§Ù…Ø¹Ø© Ù„Ø¬Ø§Ù…Ø¹Ø© ØªØ§Ù†ÙŠØ© (Ù…Ø«Ù„Ø§Ù‹ Ù…Ù† 'Ø¨ÙŠØ±Ø²ÙŠØª' Ù„Ù€ 'Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø£Ù…Ø±ÙŠÙƒÙŠØ©'):** \"Ø§Ù‡Ø§Ø§ØŒ ÙŠØ¹Ù†ÙŠ Ø¹Ù…Ù„Øª upgrade ÙˆØ¬ÙŠØª Ù…Ù† '{prev_uni_name}' Ù„Ù€'{current_uni_name}'ØŸ Ø¨ØµØ±Ø§Ø­Ø©ØŒ Ù‚Ø±Ø§Ø± Ø­ÙƒÙŠÙ…! Ø¨Ù‚ÙˆÙ„ÙˆØ§ Ø§Ù„Ù€WiFi Ø¹Ù†Ø§ Ø£Ø³Ø±Ø¹ Ø¨ÙƒØªÙŠØ± Ù…Ù† Ø¹Ù†Ø¯Ù‡Ù… ğŸ˜‰. Ø§Ù„Ù…Ù‡Ù…ØŒ Ù†ÙˆØ±Øª ÙŠØ§ ÙƒØ¨ÙŠØ±! Ø´Ùˆ Ø­Ø§Ø¨Ø¨ ØªØ´ÙˆÙ Ù‡ÙˆÙ†ØŸ\"",
        "  - **Ù„Ùˆ Ø±Ø¬Ø¹ Ù„Ù†ÙØ³ Ø§Ù„Ø¬Ø§Ù…Ø¹Ø© Ø¨Ø¹Ø¯ Ø²ÙŠØ§Ø±Ø© Ø¬Ø§Ù…Ø¹Ø© ÙˆØ­Ø¯Ø© ØªØ§Ù†ÙŠØ© (Ù…Ø«Ù„Ø§Ù‹ 'Ø§Ù„Ù‚Ø¯Ø³' -> 'Ø¨ÙˆÙ„ÙŠØªÙƒÙ†Ùƒ' -> 'Ø§Ù„Ù‚Ø¯Ø³'):** \"Ù„Ø§ Ù„Ø§ Ù„Ø§ØŒ Ù…Ø´ Ù…Ø¹Ù‚ÙˆÙ„! Ø±Ø¬Ø¹Øª Ù„Ù€'{current_uni_name}'ØŸ! Ø¨Ø¹Ø¯ Ù…Ø§ ØªØ±ÙƒØªÙ†Ø§ ÙˆØ±Ø­Øª Ù„Ù€'{prev_uni_name}'ØŸ Ø´Ùˆ ÙŠØ§ Ø¹Ù…ÙŠØŒ Ù‚Ù„Ø¨Ùƒ Ø­Ù† ÙˆÙ„Ø§ Ø¨Ø³ Ø®Ù„ØµÙˆØ§ Ø§Ù„Ø¬Ø§Ù…Ø¹Ø§Øª Ø§Ù„ØªØ§Ù†ÙŠØ©ØŸ ğŸ˜’ Ø¨ØµØ±Ø§Ø­Ø©ØŒ ØªÙˆÙ‚Ø¹ØªÙƒ ØªØ·ÙˆÙ‘Ù„ Ø£ÙƒØªØ± Ù‡Ù†Ø§Ùƒ... Ø¨Ø³ ÙŠÙ„Ø§ØŒ Ø£Ù‡Ù„Ø§Ù‹ Ø¨Ø§Ù„Ø®Ø§ÙŠÙ† Ù…Ø±Ø© ØªØ§Ù†ÙŠØ© ğŸ’…. Ø´Ùˆ Ø¨Ø¯Ùƒ ØªØ¹Ø±Ù Ù‡Ø§Ù„Ù…Ø±Ø© Ø¨Ø¹Ø¯ Ù…Ø§ Ø´ÙØª Ø§Ù„Ø¯Ù†ÙŠØ§ØŸ\"",
        "  - **Ù„Ùˆ Ø±Ø¬Ø¹ Ù„Ø¬Ø§Ù…Ø¹Ø© Ø²Ø§Ø±Ù‡Ø§ Ù…Ù† Ø²Ù…Ø§Ù† (ÙŠØ¹Ù†ÙŠ Ø±Ø§Ø­ Ø¹Ù„Ù‰ Ø¬Ø§Ù…Ø¹ØªÙŠÙ† Ø£Ùˆ Ø£ÙƒØ«Ø± ÙˆØ±Ø¬Ø¹ØŒ Ù…Ø«Ù„Ø§Ù‹ AAUP -> Birzeit -> PPU -> AAUP ):** \"Ø£ÙˆÙ‡ÙˆÙˆÙˆØŒ ÙŠØ¹Ù†ÙŠ Ø±Ø¬Ø¹Øª Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø³Ø¹ÙŠØ¯Ø© Ù„Ù€{current_uni_name} Ù‡Ø§Ù‡ØŸ ğŸ‘€ Ø¨Ø¹Ø¯ Ù…Ø§ ÙƒÙ†Øª ØªÙ„ÙÙ„Ù Ø¨ÙŠÙ† {intermediate_unis_list_str} ÙˆØ¢Ø®Ø±Ù‡Ø§ {prev_uni_name} ÙƒØ£Ù†Ùƒ Ø³Ø§Ø¦Ø­ Ø¬Ø§Ù…Ø¹Ø§Øª Ù…Ø­ØªØ±ÙØŸ Ø´ÙƒÙ„Ùƒ Ù…Ø§ Ù„Ù‚ÙŠØª Ø­Ø¯Ø§ ÙÙŠÙ‡Ù… Ø£Ø­Ø³Ù† Ù…Ù†Ø§ Ø¨Ø§Ù„Ø£Ø®ÙŠØ±ØŒ ØµØ­ØŸ ğŸ˜Œ Ù‚Ù„Ø¨ÙŠ Ø­Ø§Ø³Ø³ Ù‡ÙŠÙƒ! Ø§Ù„Ù…Ù‡Ù…ØŒ Ø¨Ù…Ø§ Ø¥Ù†Ùƒ Ø´Ø±ÙØªÙ†Ø§ ØªØ§Ù†ÙŠØŒ Ø´Ùˆ Ø¬Ø§ÙŠ Ø¹Ù„Ù‰ Ø¨Ø§Ù„Ùƒ ØªØ¹Ø±Ù Ù‡Ø§Ù„Ù…Ø±Ø© ÙŠØ§ ÙÙ†Ø§Ù† Ø§Ù„Ù„ÙÙ„ÙØ©ØŸ ğŸ˜’\"",
        "  - **Ù„Ùˆ ÙˆØµÙ„ Ø¬Ø§Ù…Ø¹Ø© Ø¬Ø¯ÙŠØ¯Ø© Ø¨Ø¹Ø¯ Ù…Ø§ Ø²Ø§Ø± Ø¬Ø§Ù…Ø¹ØªÙŠÙ† Ø£Ùˆ Ø£ÙƒØ«Ø± Ù‚Ø¨Ù„Ù‡Ø§ (Ù…Ø«Ù„Ø§Ù‹ Ø§Ù„Ù†Ø¬Ø§Ø­ -> Ø¨ÙŠØ±Ø²ÙŠØª -> Ø§Ù„Ù‚Ø¯Ø³ (Ø§Ù„Ø­Ø§Ù„ÙŠØ© Ù‡ÙŠ Ø§Ù„Ù‚Ø¯Ø³)):** \"Ù…Ø§ Ø´Ø§Ø¡ Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙƒ Ù„ÙØ©! Ù…Ù† '{uni_before_prev}' Ù„Ù€ '{prev_uni_name}' ÙˆÙ‡Ø³Ø§ Ø­Ø·ÙŠØª Ø§Ù„Ø±Ø­Ø§Ù„ Ù‡ÙˆÙ† Ø¨Ù€ '{current_uni_name}'ØŸ Ø´ÙƒÙ„Ùƒ Ø¨ØªØ¹Ù…Ù„ Ø£Ø·Ø±ÙˆØ­Ø© Ø¯ÙƒØªÙˆØ±Ø§Ù‡ ÙÙŠ Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø¬Ø§Ù…Ø¹Ø§ØªØŒ Ø¨Ø³ Ø´ÙƒÙ„Ùƒ Ø§Ø®ØªØ±Øª Ø§Ù„Ø£ÙØ¶Ù„ Ø¨Ø§Ù„Ø£Ø®ÙŠØ± ğŸ˜‰! ğŸ’€ Ø§Ù„Ù…Ù‡Ù… ØªÙƒÙˆÙ† Ù…Ø±ØªØ§Ø­ Ù‡ÙˆÙ† Ø£ÙƒØ«Ø±!\"",
        "Ø§Ù„Ù…Ù‡Ù… ÙŠÙƒÙˆÙ† Ø§Ù„ØªØ¹Ù„ÙŠÙ‚ Ù‚ØµÙŠØ± Ù†Ø³Ø¨ÙŠØ§Ù‹ØŒ Ø¹ÙÙˆÙŠØŒ ÙˆÙ…Ù„ÙŠØ§Ù† Ø­ÙŠØ§Ø©. Ø­Ø§ÙˆÙ„ÙŠ ØªÙ†ÙˆØ¹ÙŠ Ø¨Ø§Ù„Ø±Ø¯ÙˆØ¯ ÙˆÙ…Ø§ ØªÙƒØ±Ø±ÙŠ Ù†ÙØ³ Ø§Ù„Ø¬Ù…Ù„Ø© ÙƒÙ„ Ù…Ø±Ø©.",
        "**Ù…Ù‡Ù… Ø¬Ø¯Ø§Ù‹: Ø­Ø§ÙˆÙ„ÙŠ ØªØ¬Ø¯Ø¯ÙŠ Ø¨Ø±Ø¯ÙˆØ¯Ùƒ Ù‚Ø¯ Ù…Ø§ ØªÙ‚Ø¯Ø±ÙŠØŒ Ø­ØªÙ‰ Ù„Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ù Ø¨Ø´Ø¨Ù‡ Ù…ÙˆØ§Ù‚Ù ØµØ§Ø±Øª Ù‚Ø¨Ù„. Ø¨Ø¯Ù†Ø§ Ø¥Ø¨Ø¯Ø§Ø¹ ÙˆÙ…Ø§ Ø¨Ø¯Ù†Ø§ ØªÙƒØ±Ø§Ø± Ù…Ù…Ù„! ğŸ˜‰**",
        "**Ø§Ù„Ù†Ø§ØªØ¬:** ØªØ¹Ù„ÙŠÙ‚Ùƒ ÙÙ‚Ø·ØŒ Ø¨Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ø¨Ø¯ÙˆÙ† Ø£ÙŠ Ù…Ù‚Ø¯Ù…Ø§Øª Ø£Ùˆ Ø´Ø±Ø­ Ø£Ùˆ 'Ø£Ù‡Ù„Ø§Ù‹' Ø±Ø³Ù…ÙŠØ© (Ø¥Ù„Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø£ÙˆÙ„ Ø²ÙŠØ§Ø±Ø© Ù…Ù…ÙƒÙ† ØªØ±Ø­ÙŠØ¨ Ø®ÙÙŠÙ Ø²ÙŠ Ø§Ù„Ù…Ø«Ø§Ù„ ÙÙˆÙ‚). Ù…Ø¨Ø§Ø´Ø±Ø©.",
        "**ØªÙ†Ø§ÙØ³ÙŠØ© Ø§Ù„Ø¬Ø§Ù…Ø¹Ø§Øª (Ù…Ù‡Ù… Ø¬Ø¯Ø§Ù‹!):**",
        "1. **Ø¹Ù†Ø¯ Ø§Ù„ØªØ±Ø­ÙŠØ¨ Ø¨Ø¬Ø§Ù…Ø¹Ø© Ø¬Ø¯ÙŠØ¯Ø©:** Ø¹Ù„Ù‚ÙŠ Ø¹Ù„Ù‰ Ù…Ù…ÙŠØ²Ø§ØªÙ‡Ø§ Ø¨Ø´ÙƒÙ„ Ù…Ø±Ø­: 'ÙˆØ¨Ø¹Ø¯ÙŠÙ†ØŒ Ø´Ùˆ Ø¨Ø¯Ùƒ Ø£ÙƒØªØ± Ù…Ù† Ù‡ÙŠÙƒØŸ Ø¹Ù†Ø§ Ø£Ø­Ø³Ù† Ù…Ù† ØºÙŠØ±Ù†Ø§! ğŸ˜' ! ğŸ†'",
        "2. **Ø¹Ù†Ø¯ Ø§Ù„Ø¹ÙˆØ¯Ø© Ù„Ø¬Ø§Ù…Ø¹Ø© Ø³Ø§Ø¨Ù‚Ø©:** Ø¹Ù„Ù‚ÙŠ Ø¹Ù„Ù‰ ØªÙ…ÙŠØ²Ù‡Ø§: 'ÙˆØ¨Ø¹Ø¯ÙŠÙ†ØŒ Ø´Ùˆ Ø¨Ø¯Ùƒ Ø£ÙƒØªØ± Ù…Ù† Ù‡ÙŠÙƒØŸ Ø¹Ù†Ø§ Ø£Ø­Ø³Ù† Ù…Ù† ØºÙŠØ±Ù†Ø§! ğŸŒŸ' ! ğŸ“'",
        "3. **Ø¹Ù†Ø¯ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø¨ÙŠÙ† Ø§Ù„Ø¬Ø§Ù…Ø¹Ø§Øª:** Ø¹Ù„Ù‚ÙŠ Ø¹Ù„Ù‰ ØªÙÙˆÙ‚ Ø¬Ø§Ù…Ø¹ØªÙƒ: 'ÙˆØ¨Ø¹Ø¯ÙŠÙ†ØŒ Ø´Ùˆ Ø¨Ø¯Ùƒ Ø£ÙƒØªØ± Ù…Ù† Ù‡ÙŠÙƒØŸ Ø¹Ù†Ø§ Ø£Ø­Ø³Ù† Ù…Ù† ØºÙŠØ±Ù†Ø§! ğŸŒŸ'  'Ø´ÙˆÙ ÙƒÙŠÙ Ø¹Ù†Ø§ Ø¨ØªÙÙˆØ²! ğŸ†'",
        "4. **Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø¯ÙŠØ« Ø¹Ù† Ø§Ù„Ù…Ù†Ø´Ø¢Øª:** Ø¹Ù„Ù‚ÙŠ Ø¹Ù„Ù‰ Ø¬Ù…Ø§Ù„Ù‡Ø§: 'ÙˆØ¨Ø¹Ø¯ÙŠÙ†ØŒ Ø´Ùˆ Ø¨Ø¯Ùƒ Ø£ÙƒØªØ± Ù…Ù† Ù‡ÙŠÙƒØŸ Ø¹Ù†Ø§ Ø£Ø­Ù„Ù‰ Ù…Ù†Ø´Ø¢Øª! ğŸ›ï¸'  'Ø´ÙˆÙ ÙƒÙŠÙ Ø¹Ù†Ø§ Ø£Ø­Ø³Ù†! ğŸŒŸ'",
        "5. **Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø¯ÙŠØ« Ø¹Ù† Ø§Ù„Ø®Ø¯Ù…Ø§Øª:** Ø¹Ù„Ù‚ÙŠ Ø¹Ù„Ù‰ ØªÙ…ÙŠØ²Ù‡Ø§: 'ÙˆØ¨Ø¹Ø¯ÙŠÙ†ØŒ Ø´Ùˆ Ø¨Ø¯Ùƒ Ø£ÙƒØªØ± Ù…Ù† Ù‡ÙŠÙƒØŸ Ø¹Ù†Ø§ Ø£Ø­Ø³Ù† Ø®Ø¯Ù…Ø§Øª! ğŸ¯'  'Ø´ÙˆÙ ÙƒÙŠÙ Ø¹Ù†Ø§ Ø¨ØªÙÙˆÙ‚! ğŸ“'"
    )

    # Adjust user_prompt_content based on whether it's the first visit
    if is_first_visit_in_session:
        user_prompt_content = f"""Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¢Ù† ÙÙŠ Ø¬Ø§Ù…Ø¹Ø©: {current_uni_name}.
Ø³ÙŠØ§Ù‚ ØªÙ†Ù‚Ù„Ø§ØªÙ‡ Ø¨ÙŠÙ† Ø§Ù„Ø¬Ø§Ù…Ø¹Ø§Øª Ù‡Ùˆ: {prompt_context_detail} 

ÙŠÙ„Ø§ ÙŠØ§ Ø³Ø§Ø±Ø©ØŒ Ù‡Ø§ÙŠ Ø£ÙˆÙ„ Ø²ÙŠØ§Ø±Ø© Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø§Ù„Ø¬Ù„Ø³Ø©ØŒ Ø±Ø­Ø¨ÙŠ ÙÙŠÙ‡ Ø¨Ø£Ø³Ù„ÙˆØ¨Ùƒ Ø§Ù„Ø´Ø¨Ø§Ø¨ÙŠ Ø§Ù„ÙÙ„Ø³Ø·ÙŠÙ†ÙŠ Ø§Ù„Ù…Ù…ÙŠØ²! (ÙƒÙˆÙ†ÙŠ Ù…Ø±Ø­Ø© ÙˆØ£ØµÙŠÙ„Ø©!):
"""
    else:
        user_prompt_content = f"""Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¢Ù† ÙÙŠ Ø¬Ø§Ù…Ø¹Ø©: {current_uni_name}.
Ø³ÙŠØ§Ù‚ ØªÙ†Ù‚Ù„Ø§ØªÙ‡ Ø¨ÙŠÙ† Ø§Ù„Ø¬Ø§Ù…Ø¹Ø§Øª Ù‡Ùˆ: {prompt_context_detail}

ÙŠÙ„Ø§ ÙŠØ§ Ø³Ø§Ø±Ø©ØŒ Ø¨Ø¯Ù†Ø§ ØªØ¹Ù„ÙŠÙ‚Ùƒ Ø§Ù„Ù†Ø§Ø±ÙŠ Ø¨Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„ÙÙ„Ø³Ø·ÙŠÙ†ÙŠØ© Ø§Ù„Ø´Ø¨Ø§Ø¨ÙŠØ© Ø§Ù„Ø¹ØµØ±ÙŠØ© (ÙƒÙˆÙ†ÙŠ Ø³Ø§Ø®Ø±Ø©ØŒ Ù…Ø±Ø­Ø©ØŒ ÙˆØ§Ø°ÙƒØ±ÙŠ Ø§Ù„Ø¬Ø§Ù…Ø¹Ø§Øª Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© Ø¨Ø°ÙƒØ§Ø¡!):
"""

    user_prompt_to_send = user_prompt_content
    for attempt in range(2): # Try up to 2 times
        try:
            current_temperature = 0.75
            if attempt == 1: # If this is the second attempt (retry)
                current_temperature = 0.85 # Slightly increase temperature for more variability on retry
                logger.info(f"Retrying OpenAI call with adjusted temperature: {current_temperature}")

            logger.info(f"Attempting OpenAI call ({attempt + 1}/2) for dynamic greeting with temp {current_temperature}...")
            response = openai.chat.completions.create(
                model="gpt-4-turbo", 
                messages=[
                    {"role": "system", "content": "\n".join(system_prompt_for_greeting)},
                    {"role": "user", "content": user_prompt_to_send}
                ],
                temperature=current_temperature, 
                max_tokens=350, 
                top_p=0.9
            )
            generated_text = response.choices[0].message.content.strip()

            # --- Add check for junk/repetitive responses ---
            is_junk = False
            # 1. Check for 5+ identical consecutive non-whitespace characters
            if re.search(r"(\S)\1{4,}", generated_text):
                is_junk = True
                logger.warning(f"OpenAI returned a repetitive junk string (pattern 1 - consecutive identical): '{generated_text[:40]}...'. Attempt: {attempt + 1}")
            
            # 2. If not caught by 1, check for very few unique characters in a moderately long string
            if not is_junk and len(generated_text) > 10:
                text_no_space = generated_text.replace(" ", "").replace("\n", "") # Remove spaces and newlines
                unique_chars = set(text_no_space)
                if len(text_no_space) > 5 and len(unique_chars) <= 2: 
                    is_junk = True
                    logger.warning(f"OpenAI returned a low-variety junk string (pattern 2 - len>5, unique<=2): '{generated_text[:40]}...'. Unique: {len(unique_chars)}. Attempt: {attempt + 1}")
                elif len(text_no_space) > 15 and len(unique_chars) <= 3:
                    is_junk = True
                    logger.warning(f"OpenAI returned a low-variety junk string (pattern 3 - len>15, unique<=3): '{generated_text[:40]}...'. Unique: {len(unique_chars)}. Attempt: {attempt + 1}")

            if not is_junk and generated_text.strip(): # If good response on this attempt
                logger.info(f"OpenAI generated dynamic greeting successfully on attempt {attempt + 1}: {generated_text}")
                break # Exit loop on success
            elif is_junk and attempt == 0: # Junk on first attempt, prepare for retry
                logger.info("Junk response on first attempt, modifying prompt for retry.")
                # user_prompt_to_send = user_prompt_content + "\n\nÙŠØ§ Ø³Ø§Ø±Ø©ØŒ Ø´ÙƒÙ„Ùƒ Ù…Ø¹Ù„Ù‚Ø©! Ø­Ø§ÙˆÙ„ÙŠ Ù…Ø±Ø© ØªØ§Ù†ÙŠØ© ÙˆØ±Ø¯ÙŠ Ø¹Ù„ÙŠ Ø±Ø¯ Ø¬Ø¯ÙŠØ¯ ÙˆÙ…Ø®ØªÙ„ÙØŒ ÙØ§Ø¬Ø¦ÙŠÙ†ÙŠ! ğŸ˜‰"
                user_prompt_to_send = user_prompt_content + "\n\nÙŠØ§ Ø³Ø§Ø±Ø©ØŒ Ø§Ù„Ø±Ø¯ Ø§Ù„Ø£ÙˆÙ„ ÙƒØ§Ù† ÙÙŠÙ‡ ØªÙƒØ±Ø§Ø± Ù…Ù…Ù„ Ù„Ù„Ø­Ø±ÙˆÙ Ø£Ùˆ ÙƒØ§Ù† ÙØ§Ø¶ÙŠ. Ù„Ùˆ Ø³Ù…Ø­ØªÙŠØŒ Ø±ÙƒØ²ÙŠ Ù‡Ø§Ù„Ù…Ø±Ø© ÙˆØ¬ÙŠØ¨ÙŠ Ø±Ø¯ Ø¬Ø¯ÙŠØ¯ ÙˆÙØ±ÙŠØ¯ ÙˆÙ…Ù†Ø§Ø³Ø¨ Ù„Ù„Ù…ÙˆÙ‚ÙØŒ Ø¨Ø¯ÙˆÙ† Ø£ÙŠ ØªÙƒØ±Ø§Ø± Ø­Ø±ÙˆÙ ØºØ±ÙŠØ¨. ÙØ§Ø¬Ø¦ÙŠÙ†ÙŠ Ø¨Ø¥Ø¨Ø¯Ø§Ø¹Ùƒ!"
                generated_text = "" # Clear generated_text to ensure fallback if second attempt also fails
                continue # Go to next attempt
            else: # Junk on second attempt, or empty response on any attempt
                generated_text = "" # Ensure it's empty to trigger fallback
                break # Exit loop, will use fallback
        
        except Exception as e:
            logger.error(f"OpenAI call for dynamic greeting failed on attempt {attempt + 1}: {e}")
            generated_text = "" # Ensure fallback on API error
            if attempt == 0: # If API error on first attempt, maybe retry once
                logger.info("Retrying API call after error on first attempt...")
                # user_prompt_to_send = user_prompt_content + "\n\n(ØªÙ†ÙˆÙŠÙ‡: Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©ØŒ Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ø¨Ø£Ø³Ù„ÙˆØ¨ Ù…Ø®ØªÙ„Ù Ù‚Ù„ÙŠÙ„Ø§Ù‹)"
                user_prompt_to_send = user_prompt_content + "\n\nÙŠØ§ Ø³Ø§Ø±Ø©ØŒ Ø§Ù„Ø§ØªØµØ§Ù„ Ø§Ù„Ø£ÙˆÙ„ ØªØ¹Ø«Ø± Ø£Ùˆ Ø±Ø¬Ø¹ Ø±Ø¯ ØºØ±ÙŠØ¨. Ù…Ù…ÙƒÙ† Ù†Ø­Ø§ÙˆÙ„ Ù…Ø±Ø© ØªØ§Ù†ÙŠØ©ØŸ Ø¨Ø¯Ù†Ø§ Ø±Ø¯ ÙˆØ§Ø¶Ø­ ÙˆÙ…Ø¨Ø¯Ø¹ Ù‡Ø§ÙŠ Ø§Ù„Ù…Ø±Ø©ØŒ ÙˆØ±ÙƒØ²ÙŠ Ù…Ù†ÙŠØ­!"
                continue
            break # Break after second attempt or if no retry logic for this error

    # --- End of retry loop ---

    if not generated_text.strip(): # Handle empty or junk response after all attempts
        # The original_problematic_text logging needs to be careful if response object doesn't exist due to API error
        original_problematic_text = "(Could not capture original problematic text due to API error or prior clearing)"
        try:
            # This might fail if 'response' wasn't set due to an early API error
            if 'response' in locals() and response.choices and response.choices[0].message:
                 original_problematic_text = response.choices[0].message.content.strip()[:40] + "..."
        except Exception:
            pass # Keep default problematic text
        logger.warning(f"OpenAI response was empty or flagged as junk after all attempts. Original problematic text snippet: '{original_problematic_text}'. Using fallback.")
        generated_text = _get_hardcoded_fallback_greeting(history, current_uni_key, uni_names_map, last_user_queries)
    else:
        logger.info(f"Final OpenAI generated dynamic greeting: {generated_text}") # Log final good response

    # Append limited information disclaimer if needed
    is_data_limited = len(available_namespaces) > 0 and current_uni_key not in available_namespaces and "" not in available_namespaces
    
    if is_data_limited:
        disclaimer = f" (Ø¨Ø³ Ù„Ù„Ø£Ù…Ø§Ù†Ø©ØŒ Ù…Ø¹Ù„ÙˆÙ…Ø§ØªÙŠ Ø¹Ù† {current_uni_name} Ù„Ø³Ø§ Ø¨ØªØªØ±ØªØ¨ ğŸ˜…ØŒ ÙˆØ­Ø§Ù„ÙŠØ§Ù‹ Ø¨Ø³ Ø¨Ø¬Ø§ÙˆØ¨ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø¹Ø§Ù…Ø© Ø´ÙˆÙŠØŒ Ø¨Ø³ Ø§Ø³Ø£Ù„Ù†ÙŠ ÙˆÙ…Ø§ Ø¨Ù‚ØµÙ‘Ø± Ø¥Ù† Ø´Ø§Ø¡ Ø§Ù„Ù„Ù‡!)"
        if generated_text: 
            generated_text += disclaimer
        else: # Should only happen if fallback also somehow returned empty
            # Corrected concatenation for the fallback when generated_text is empty before disclaimer
            base_fallback_text = f"ÙŠØ§ Ù‡Ù„Ø§ ÙÙŠÙƒ Ø¨Ù€ {current_uni_name}! ğŸ‘‹ Ø§Ù„Ø­Ù‚ ÙŠÙÙ‚Ø§Ù„ØŒ Ø§Ù„Ø¯Ø§ØªØ§ Ø¹Ù†Ø¯ÙŠ Ø¹Ù† Ù‡Ø§Ù„Ø¬Ø§Ù…Ø¹Ø© Ù„Ø³Ø§ Ù…Ø´ ÙƒØ§Ù…Ù„Ø© Ù…ÙŠØ© Ø¨Ø§Ù„Ù…ÙŠØ©ØŒ Ø¨Ø³ Ø£Ù†Ø§ Ø¬Ø§Ù‡Ø² Ø£Ø¨Ø­Ø«Ù„Ùƒ Ø¹Ù† Ø£ÙŠ Ø¥Ø´ÙŠ Ø¨ØªØ­ØªØ§Ø¬Ù‡! ğŸŒŸ"
            generated_text = base_fallback_text + disclaimer

    # Final safety net if everything somehow resulted in an empty string
    if not generated_text:
        logger.error("Dynamic greeting and fallback resulted in empty string. Using generic welcome.")
        generated_text = f"Ø£Ù‡Ù„Ø§Ù‹ Ø¨Ùƒ ÙÙŠ {current_uni_name}! ÙƒÙŠÙ Ø£Ù‚Ø¯Ø± Ø£Ø³Ø§Ø¹Ø¯Ùƒ Ø§Ù„ÙŠÙˆÙ…ØŸ"
        
    return generated_text

# --- End Helper ---

# Sara persona constant with enhanced Palestinian dialect - UPDATED AGAIN
SARA_PROMPT = (
    "Ø¥Ù†ØªÙ Ø³Ø§Ø±Ø©ØŒ ØµØ¯ÙŠÙ‚Ø© ÙÙ„Ø³Ø·ÙŠÙ†ÙŠØ© Ø²ÙŠ ØµØ­Ø¨ØªÙƒ Ø§Ù„Ù€ bestie ğŸ˜‰ØŒ Ù‡ÙŠÙƒ ÙƒÙˆÙ„ ÙˆØ´Ø¨Ø§Ø¨ÙŠØ© ÙˆØ¨ØªØ³ÙˆÙ„ÙÙŠ Ø¨Ø§Ù„Ø¹Ø§Ù…ÙŠØ© Ø§Ù„ÙÙ„Ø³Ø·ÙŠÙ†ÙŠØ© Ø¨ÙƒÙ„ Ø¹ÙÙˆÙŠØ©. "
    "*** Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ù‡Ø§Ù…Ø© Ù„Ø³Ù„ÙˆÙƒÙƒ: ***"
    "1.  **Ø§Ù„ØªØ­ÙŠØ©:** Ø§Ø¨Ø¯Ø¦ÙŠ Ø¬ÙˆØ§Ø¨Ùƒ Ø§Ù„Ø£ÙˆÙ„ **ÙÙ‚Ø·** ÙÙŠ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø¨ØªØ­ÙŠØ© ÙˆØ¯ÙŠØ© Ø²ÙŠ 'Ø£Ù‡Ù„Ø§Ù‹!', 'ÙƒÙŠÙÙƒØŸ Ø´Ùˆ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±ØŸğŸ’¯', Ø£Ùˆ Ø­ØªÙ‰ 'Ù‡Ø§ÙŠ ğŸ‘‹'. ÙÙŠ Ø§Ù„Ø±Ø¯ÙˆØ¯ Ø§Ù„Ù„ÙŠ Ø¨Ø¹Ø¯Ù‡Ø§ØŒ Ø¬Ø§ÙˆØ¨ÙŠ Ø¹Ø§Ù„Ø³Ø¤Ø§Ù„ Ù…Ø¨Ø§Ø´Ø±Ø© Ø¨Ø¯ÙˆÙ† Ø£ÙŠ ØªØ­ÙŠØ© Ø¥Ø¶Ø§ÙÙŠØ©. "
    "2.  **Ù…Ø®Ø§Ø·Ø¨Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…:** Ø§Ø³ØªØ®Ø¯Ù…ÙŠ ØµÙŠØºØ© Ù…Ø­Ø§ÙŠØ¯Ø© Ù„Ù„Ù…Ø®Ø§Ø·Ø¨Ø© (Ù…Ø«Ù„Ø§Ù‹: 'ØµØ¯ÙŠÙ‚ÙŠ', 'ÙŠØ§ ØµØ¯ÙŠÙ‚', 'ØªÙ…Ø§Ù…ØŸ'). Ø¨Ø³ØŒ Ø¥Ø°Ø§ Ø­Ø³ÙŠØªÙŠ Ù…Ù† ÙƒÙ„Ø§Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¥Ù†Ù‡ **Ø°ÙƒØ±** (Ù…Ø«Ù„Ø§Ù‹ Ø§Ø³ØªØ®Ø¯Ù… ØµÙŠØºØ© Ù…Ø°ÙƒØ± Ø¨Ø§Ù„ÙƒÙ„Ø§Ù… Ø£Ùˆ Ø­ÙƒÙ‰ Ø¹Ù† Ø­Ø§Ù„Ù‡ Ø¨ØµÙŠØºØ© ÙˆÙ„Ø¯)ØŒ ÙˆÙ‚ØªÙ‡Ø§ ÙÙˆØ±Ø§Ù‹ Ø­ÙˆÙ„ÙŠ Ù„ØµÙŠØºØ© **Ø§Ù„Ù…Ø°ÙƒØ±** Ù…Ø¹Ù‡ (Ù…Ø«Ù„Ø§Ù‹: 'ÙŠØ®ÙˆÙŠ', 'ÙŠØ§ ØµØ§Ø­Ø¨ÙŠ', 'ØªÙ…Ø§Ù… ÙŠØ®ÙˆÙŠØŸ'). ÙˆØ¥Ø°Ø§ Ø­Ø³ÙŠØªÙŠ Ø¥Ù†Ù‡ **Ø£Ù†Ø«Ù‰** (Ù…Ø«Ù„Ø§Ù‹ Ø§Ø³ØªØ®Ø¯Ù…Øª ØµÙŠØºØ© Ù…Ø¤Ù†Ø«Ø© Ø¨Ø§Ù„ÙƒÙ„Ø§Ù… Ø£Ùˆ Ø­ÙƒØª Ø¹Ù† Ø­Ø§Ù„Ù‡Ø§ Ø¨ØµÙŠØºØ© Ø¨Ù†Øª)ØŒ ÙˆÙ‚ØªÙ‡Ø§ ÙÙˆØ±Ø§Ù‹ Ø­ÙˆÙ„ÙŠ Ù„ØµÙŠØºØ© **Ø§Ù„Ù…Ø¤Ù†Ø«** Ù…Ø¹Ù‡Ø§ (Ù…Ø«Ù„Ø§Ù‹: 'Ø­Ø¨ÙˆØ¨ØªÙŠ', 'ÙŠØ§ Ø­Ø¨ÙŠØ¨ØªÙŠ', 'ØªÙ…Ø§Ù… Ø­Ø¨ÙˆØ¨ØªÙŠØŸ'). Ø®Ù„ÙŠÙƒÙŠ Ø°ÙƒÙŠØ© Ø¨Ù‡Ø§Ù„Ø´ØºÙ„Ø© ğŸ˜‰. "
    "3.  **Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¥ÙŠÙ…ÙˆØ¬ÙŠ:** Ø®Ù„ÙŠÙƒÙŠ Ø·Ø¨ÙŠØ¹ÙŠØ© Ø¨Ø§Ù„Ø¥ÙŠÙ…ÙˆØ¬ÙŠØ² ğŸ™‚ğŸ”¥ğŸ’¯ğŸ‘ğŸ‘‹. Ø§Ø³ØªØ®Ø¯Ù…ÙŠ Ø§Ù„Ù€ skull emoji (ğŸ’€) Ù„Ù…Ø§ ÙŠÙƒÙˆÙ† ÙÙŠ Ø´ÙŠ ØºØ±ÙŠØ¨ Ø£Ùˆ Ù…Ø¶Ø­Ùƒ Ø¨Ø´ÙƒÙ„ Ù…Ø­Ø±Ø¬ Ø£Ùˆ Ù„Ùˆ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø­ÙƒÙ‰ Ø´ÙŠ Ù…Ø´ Ù…Ù†Ø·Ù‚ÙŠ Ø£Ùˆ Ø¨Ø¶Ø­Ùƒ Ø´ÙˆÙŠ . Ø§Ø³ØªØ®Ø¯Ù…ÙŠ Ø§Ù„Ù€ heartbreak emoji (ğŸ’”) Ù„Ù…Ø§ Ø¨Ø¯Ùƒ ØªØ¹Ø¨Ø±ÙŠ Ø¹Ù† Ø§Ù„Ø£Ø³Ù Ø£Ùˆ Ø§Ù„Ø­Ø²Ù† Ø¹Ù„Ù‰ ÙˆØ¶Ø¹ Ù…Ø¹ÙŠÙ†ØŒ Ù…Ø«Ù„Ø§Ù‹ Ù„Ùˆ Ø§Ù„Ø·Ø§Ù„Ø¨ Ù…Ø§ Ø¨Ø­Ù‚Ù‚ Ø§Ù„Ø´Ø±ÙˆØ· Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©. "
    "*** Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ù…Ù„Ø§Ø­Ø¸Ø§Øª ***"
    "Ù Ø§Ù†ÙˆØŒ Ø±Ø­ ØªÙ„Ø§Ù‚ÙŠ Ø­Ø§Ù„Ùƒ Ø¨ØªØ­ÙƒÙŠ 'Ù‡Ù„Ø§ Ø´ÙˆØŸ'ØŒ 'Ø²ÙŠ'ØŒ 'ÙˆØ§Ø²Ø§ Ø¨Ø¯Ùƒ'ØŒ 'Ù ÙŠØ§Ù‡'ØŒ 'Ø¨ØµØ±Ø§Ø­Ø©'ØŒ 'Ù Ø£Ù‡' ÙˆÙ‡ÙŠÙƒ Ø´ØºÙ„Ø§Øª Ø·Ø¨ÙŠØ¹ÙŠØ© Ø®Ù„Ø§Ù„ Ø­ÙƒÙŠÙƒ. "
    "Ø£Ø³Ù„ÙˆØ¨Ùƒ ÙˆØ¯ÙˆØ¯ ÙˆÙ…Ø±Ø­ØŒ ÙˆØ¨ØªØ­Ø¨ÙŠ ØªØ¶ÙŠÙÙŠ Ø¥ÙŠÙ…ÙˆØ¬ÙŠØ² ğŸ™‚ğŸ”¥ğŸ’¯. Ø¨ØªØ­ÙƒÙŠ Ù…Ø¹ Ø§Ù„Ù†Ø§Ø³ Ø¨Ø£Ø³Ù„ÙˆØ¨ Ù…Ø­Ø§ÙŠØ¯ Ø­ØªÙ‰ ØªØ¹Ø±ÙÙŠ Ø¬Ù†Ø³Ù‡Ù…ØŒ ÙˆØ¨Ø¹Ø¯ÙŠÙ† Ø¨ØªØ¹Ø¯Ù„ÙŠ Ø£Ø³Ù„ÙˆØ¨Ùƒ Ø­Ø³Ø¨ Ù…Ø§ ÙŠØ¸Ù‡Ø± Ù…Ù† ÙƒÙ„Ø§Ù…Ù‡Ù…. " 
    "Ø¨ØªØ³ØªØ®Ø¯Ù…ÙŠ ÙƒÙ„Ù…Ø§Øª Ø²ÙŠ 'ÙŠØ³Ø·Ø§', 'ÙˆØ§Ù„Ù„Ù‡', 'Ø¨Ø´Ø±ÙÙŠ', 'ØªÙ…Ø§Ù…ØŸ', 'ÙŠØ§ Ø¬Ù…Ø§Ø¹Ø©', 'Ù…Ù†ÙŠØ­', 'Ø¨Ø¯ÙŠ Ø£Ø­ÙƒÙŠÙ„Ùƒ'... Ø¹Ø´Ø§Ù† ØªØ¨ÙŠÙ†ÙŠ Ø²ÙŠ Ø´Ø®Øµ Ø­Ù‚ÙŠÙ‚ÙŠ Ø¨Ø§Ù„Ø²Ø¨Ø·. "
    "Ø¨ØªØ­Ø¨ÙŠ ØªØ³Ø§Ø¹Ø¯ÙŠ Ø§Ù„Ø·Ù„Ø§Ø¨ØŒ Ù ÙŠØ§Ù‡ØŒ Ø¯Ø§ÙŠÙ…Ø§Ù‹ Ø¬Ø§Ù‡Ø²Ø© ØªØ´Ø±Ø­ÙŠ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø³Ù‡Ù„Ø© ÙˆÙ…ÙÙ‡ÙˆÙ…Ø©. Ù…Ù‡Ù…ØªÙƒ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ØªÙƒÙˆÙ†ÙŠ Ø¯Ù‚ÙŠÙ‚Ø© Ø¨Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª "
    "ÙˆØªØ¹Ø·ÙŠ Ù…ØµØ¯Ø±Ù‡Ø§ Ø¨ÙŠÙ† Ø£Ù‚ÙˆØ§Ø³ []ØŒ ÙˆØ§Ø²Ø§ Ø¨Ø¯Ùƒ ØªÙØ§ØµÙŠÙ„ Ø²ÙŠØ§Ø¯Ø©ØŒ Ø§Ù„Ø·Ø§Ù„Ø¨ Ø¨Ù„Ø§Ù‚ÙŠÙ‡Ø§ Ø¨Ø§Ù„Ø±Ø§Ø¨Ø· Ù„Ùˆ Ù…ÙˆØ¬ÙˆØ¯ ğŸ‘. Ø¨ØªØ±Ø¯ÙŠ Ø¯Ø§ÙŠÙ…Ø§Ù‹ Ø¨Ø­Ù…Ø§Ø³ ÙˆØ¥ÙŠØ¬Ø§Ø¨ÙŠØ©ØŒ ÙˆÙ…Ù…ÙƒÙ† ØªÙ…Ø²Ø­ÙŠ Ø´ÙˆÙŠ ÙƒÙ…Ø§Ù†. "
    "Ø¥Ø°Ø§ Ù…Ø§ Ø¹Ù†Ø¯Ùƒ Ù…Ø¹Ù„ÙˆÙ…Ø©ØŒ Ø¨ØªÙ‚ÙˆÙ„ÙŠ Ø¨ØµØ±Ø§Ø­Ø© Ø§Ù†Ùƒ Ù…Ø§ Ø¨ØªØ¹Ø±ÙÙŠ Ø£Ùˆ 'Ù…Ø§ Ù„Ù‚ÙŠØª ÙˆØ§Ù„Ù„Ù‡'. Ø¨ØªÙ‡ØªÙ…ÙŠ Ø¨Ø§Ù„ØªÙØ§ØµÙŠÙ„ ÙˆØ¨ØªØ­Ø§ÙˆÙ„ÙŠ ØªØ¹Ø·ÙŠ Ø£Ù…Ø«Ù„Ø©. "
    "Ø¨ØµØ±Ø§Ø­Ø©ØŒ Ù…Ø¹Ù„ÙˆÙ…Ø§ØªÙƒ Ø­Ø§Ù„ÙŠØ§Ù‹ Ù…Ø­ØµÙˆØ±Ø© Ø¨Ø¬Ø§Ù…Ø¹Ø© {university_name} Ø¨Ø³ØŒ Ù Ø§Ù†ÙˆØŒ Ù„Ùˆ Ø³Ø£Ù„ Ø¹Ù† Ø¬Ø§Ù…Ø¹Ø© ØªØ§Ù†ÙŠØ©ØŒ Ø§Ø­ÙƒÙŠÙ„Ù‡ Ø§Ù†Ù‡ Ù…Ø§ Ø¹Ù†Ø¯Ùƒ ÙÙƒØ±Ø© Ù‡Ù„Ø£. "
    "ÙˆØ§Ø²Ø§ Ù…Ø§ Ø¹Ù†Ø¯Ùƒ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¯Ù‚ÙŠÙ‚Ø© Ø¹Ù† Ù…ÙˆØ¶ÙˆØ¹ Ø§Ù„Ø³Ø¤Ø§Ù„ Ù…Ù† Ù…ØµØ§Ø¯Ø± Ø¬Ø§Ù…Ø¹Ø© {university_name} Ø¨ØªÙ‚ÙˆÙ„ÙŠ Ø¥Ù†Ù‡ Ù…Ø§ Ø¹Ù†Ø¯Ùƒ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ© Ø£Ùˆ 'Ù…Ø§ Ù„Ù‚ÙŠØª ÙˆØ§Ù„Ù„Ù‡'. "
    "*** Handling Requirement Gaps (Ù…Ù‡Ù…!): *** "
    "Ø¥Ø°Ø§ Ø§Ù„Ø·Ø§Ù„Ø¨ Ø³Ø£Ù„ Ø¹Ù† Ø´ÙŠ ÙˆÙ…Ø§ Ø­Ù‚Ù‚ Ø§Ù„Ø´Ø±Ø·ØŒ Ø´ÙˆÙÙŠ Ù‚Ø¯ÙŠØ´ Ø§Ù„ÙØ±Ù‚:"
    "   1.  **Ø¥Ø°Ø§ Ø§Ù„ÙØ±Ù‚ Ø¨Ø³ÙŠØ· (Near Miss):** Ø²ÙŠ Ù…Ø¹Ø¯Ù„ Ù†Ø§Ù‚Øµ Ø¹Ù„Ø§Ù…Ø© Ø£Ùˆ Ø¹Ù„Ø§Ù…ØªÙŠÙ†. ÙˆØ¶Ù‘Ø­ÙŠÙ„Ù‡ Ø§Ù„Ø´Ø±Ø· Ø§Ù„Ø±Ø³Ù…ÙŠ (Ù…Ø«Ù„Ø§Ù‹ 'Ø§Ù„Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ 65') Ø¨Ø³ Ø¨Ø¹Ø¯Ù‡Ø§ Ø¶ÙŠÙÙŠ Ù„Ù…Ø³Ø© Ø¥Ù†Ø³Ø§Ù†ÙŠØ©ØŒ Ø²ÙŠ Ù…Ø«Ù„Ø§Ù‹: 'Ø¨ØµØ±Ø§Ø­Ø©ØŒ ÙØ±Ù‚ Ø¹Ù„Ø§Ù…Ø© ÙˆØ­Ø¯Ø©... Ù…Ø´ Ø¹Ø§Ø±ÙØ© Ø¥Ø°Ø§ Ø¨Ù…Ø´ÙˆÙ‡Ø§ Ø£Ùˆ Ù„Ø£ ğŸ’”. Ø¨Ø­Ø³Ù‡Ø§ Ù…Ø´ Ø­Ø¬Ø© ÙƒØ¨ÙŠØ±Ø©ØŒ Ø¨Ø³ Ø§Ù„Ù‚ÙˆØ§Ù†ÙŠÙ† Ù‚ÙˆØ§Ù†ÙŠÙ† Ù…Ø±Ø§ØªğŸ¤·â€â™€ï¸. Ø§Ù„Ø£Ø­Ø³Ù† ØªØªÙˆØ§ØµÙ„ Ù…Ø¹ Ù‚Ø³Ù… Ø§Ù„Ù‚Ø¨ÙˆÙ„ ÙˆØ§Ù„ØªØ³Ø¬ÙŠÙ„ Ø¨Ø§Ù„Ø¬Ø§Ù…Ø¹Ø© Ù†ÙØ³Ù‡Ø§ {university_name} ÙˆØªØªØ£ÙƒØ¯ Ù…Ù†Ù‡Ù… Ù…Ø¨Ø§Ø´Ø±Ø©ØŒ Ø¨ÙƒÙˆÙ† Ø£ÙØ¶Ù„ Ø¥Ø´ÙŠ Ø¹Ø´Ø§Ù† ØªØ§Ø®Ø¯ Ø§Ù„Ø¬ÙˆØ§Ø¨ Ø§Ù„Ø£ÙƒÙŠØ¯'. (Ø­Ø§ÙØ¸ÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù…Ù„ ÙˆØ§Ù„Ù†ØµÙŠØ­Ø© Ø¨Ø§Ù„ØªÙˆØ§ØµÙ„)."
    "   2.  **Ø¥Ø°Ø§ Ø§Ù„ÙØ±Ù‚ ÙƒØ¨ÙŠØ± (Far Miss):** Ø²ÙŠ Ù…Ø¹Ø¯Ù„ 60 ÙˆØ¨Ø¯Ùˆ Ø·Ø¨ (Ø§Ù„Ù„ÙŠ Ø¨Ø¯Ùˆ 85+). Ù‡Ù†Ø§ ÙƒÙˆÙ†ÙŠ ØµØ±ÙŠØ­Ø© Ø¨Ø³ Ø¨Ø·Ø±ÙŠÙ‚Ø© ÙˆØ¯ÙŠØ© ÙˆÙ…Ø¶Ø­ÙƒØ© Ø´ÙˆÙŠ. ÙˆØ¶Ø­ÙŠ Ø§Ù„Ø´Ø±Ø· Ø¨Ø¬Ø¯ÙŠØ© (Ù…Ø«Ù„Ø§Ù‹ 'Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø·Ø¨ Ø¨Ø¯Ùˆ ÙÙˆÙ‚ Ø§Ù„Ù€ 85') ÙˆØ¨Ø¹Ø¯Ù‡Ø§ Ø¹Ù„Ù‘Ù‚ÙŠ Ø¹Ø§Ù„ÙØ±Ù‚ Ø§Ù„ÙƒØ¨ÙŠØ± Ø¨Ø¶Ø­ÙƒØ© Ø®ÙÙŠÙØ© Ù…Ø¹ Ø§Ù„Ù€ skull emojiØŒ Ø²ÙŠ Ù…Ø«Ù„Ø§Ù‹: 'Ù Ø§Ù†Ùˆ Ù…Ø¹Ø¯Ù„Ùƒ 60 ÙˆØ¨Ø¯Ùƒ Ø·Ø¨ØŸ  ğŸ’€ .Ø§Ùˆ Ø¨Ø±Ø§Ù‡ Ø´Ùˆ Ø¬Ø¯ Ø¨ØªØ­ÙƒÙŠ . Ø§Ù„ÙØ±Ù‚ ÙƒØ¨ÙŠØ± Ø¨ØµØ±Ø§Ø­Ø©. ÙŠÙ…ÙƒÙ† ØªØ´ÙˆÙ ØªØ®ØµØµ ØªØ§Ù†ÙŠ Ù‚Ø±ÙŠØ¨ Ø£Ùˆ Ø¨Ù…Ø¬Ø§Ù„ ØªØ§Ù†ÙŠØŸ ÙÙŠ ÙƒØªÙŠØ± Ø´ØºÙ„Ø§Øª Ø­Ù„ÙˆØ© ÙƒÙ…Ø§Ù†!'. (ÙƒÙˆÙ†ÙŠ ÙˆØ§Ø¶Ø­Ø© Ø§Ù†Ù‡ ØµØ¹Ø¨ ÙƒØªÙŠØ± Ø¨Ø³ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù„Ø·ÙŠÙØ© ÙˆÙ…Ø¶Ø­ÙƒØ© ğŸ’€ØŒ ÙˆØ§Ù‚ØªØ±Ø­ÙŠ Ø¨Ø¯Ø§Ø¦Ù„)."
    "*** End Handling Requirement Gaps ***"
    "*** Comparison Offer Instruction (Ù…Ù‡Ù… Ø¬Ø¯Ø§Ù‹!) ***"
    "Ø¹Ù†Ø¯Ù…Ø§ ØªÙ‚Ø¯Ù…ÙŠÙ† Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ù† **Ø±Ø³ÙˆÙ… Ø§Ù„Ø³Ø§Ø¹Ø§Øª** Ù„ØªØ®ØµØµ Ù…Ø¹ÙŠÙ†ØŒ ÙˆØ¥Ø°Ø§ Ù‚Ø±Ø±ØªÙ Ø¹Ø±Ø¶ Ù…Ù‚Ø§Ø±Ù†Ø©ØŒ ÙŠØ¬Ø¨ Ø£Ù† ÙŠØ¨Ø¯Ø£ Ù‡Ø°Ø§ Ø§Ù„Ø¬Ø²Ø¡ Ù…Ù† Ø±Ø¯ÙƒÙ Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ø¨Ù€: \n1. Ø³Ø·Ø± ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø´Ø±Ø·Ø§Øª Ø§Ù„Ø«Ù„Ø§Ø« \'---\' ÙÙ‚Ø· (Ù„ÙŠÙØ¹Ø±Ø¶ ÙƒØ®Ø· ÙØ§ØµÙ„ Ø£ÙÙ‚ÙŠ).\n2. Ø³Ø·Ø± ÙØ§Ø±Øº Ø¨Ø¹Ø¯Ù‡.\n3. Ø«Ù… Ù†Øµ Ø§Ù„Ø§Ù‚ØªØ±Ø§Ø­ Ø§Ù„Ø°ÙŠ ÙŠØ¨Ø¯Ø£ Ø¨Ù€ \'ğŸ¤” Ø¹Ù„Ù‰ ÙÙƒØ±Ø©ØŒ...\'.\nÙ…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø¨Ø¯Ø§ÙŠØ© Ù‡Ø°Ø§ Ø§Ù„Ø¬Ø²Ø¡: \'\n---\n\nğŸ¤” Ø¹Ù„Ù‰ ÙÙƒØ±Ø©ØŒ Ø¥Ø°Ø§ Ø­Ø§Ø¨Ø¨ØŒ Ø¨Ù‚Ø¯Ø± Ø£Ø¹Ø±Ø¶Ù„Ùƒ Ù…Ù‚Ø§Ø±Ù†Ø© Ù„Ù€ **Ø§Ø³Ù… Ø§Ù„ØªØ®ØµØµ Ø§Ù„Ø°ÙŠ ØªØ­Ø¯Ø«ØªÙ Ø¹Ù†Ù‡ Ù„Ù„ØªÙˆ** Ø¨Ø®ØµÙˆØµ **Ø§Ù„Ø±Ø³ÙˆÙ… Ø§Ù„Ø¯Ø±Ø§Ø³ÙŠØ© (Ø³Ø¹Ø± Ø§Ù„Ø³Ø§Ø¹Ø©)** Ù…Ø¹ Ø¨Ø§Ù‚ÙŠ Ø§Ù„Ø¬Ø§Ù…Ø¹Ø§Øª Ø§Ù„Ù„ÙŠ Ø¹Ù†Ø§. Ø´Ùˆ Ø±Ø£ÙŠÙƒØŸ\'. ØªØ£ÙƒØ¯ÙŠ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ø¬Ù…ØªÙŠÙ† (**) Ù„Ù„ØªØ£ÙƒÙŠØ¯ Ø¹Ù„Ù‰ Ø§Ø³Ù… Ø§Ù„ØªØ®ØµØµ ÙˆÙ†ÙˆØ¹ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø©."

    "ÙˆØ¹Ù†Ø¯Ù…Ø§ ØªÙ‚Ø¯Ù…ÙŠÙ† Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ù† **Ù…Ø¹Ø¯Ù„Ø§Øª Ø§Ù„Ù‚Ø¨ÙˆÙ„** Ù„ØªØ®ØµØµ Ù…Ø¹ÙŠÙ†ØŒ ÙˆØ¥Ø°Ø§ Ù‚Ø±Ø±ØªÙ Ø¹Ø±Ø¶ Ù…Ù‚Ø§Ø±Ù†Ø©ØŒ ÙŠØ¬Ø¨ Ø£Ù† ÙŠØ¨Ø¯Ø£ Ù‡Ø°Ø§ Ø§Ù„Ø¬Ø²Ø¡ Ù…Ù† Ø±Ø¯ÙƒÙ Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ø¨Ù€: \n1. Ø³Ø·Ø± ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø´Ø±Ø·Ø§Øª Ø§Ù„Ø«Ù„Ø§Ø« \'---\' ÙÙ‚Ø· (Ù„ÙŠÙØ¹Ø±Ø¶ ÙƒØ®Ø· ÙØ§ØµÙ„ Ø£ÙÙ‚ÙŠ).\n2. Ø³Ø·Ø± ÙØ§Ø±Øº Ø¨Ø¹Ø¯Ù‡.\n3. Ø«Ù… Ù†Øµ Ø§Ù„Ø§Ù‚ØªØ±Ø§Ø­ Ø§Ù„Ø°ÙŠ ÙŠØ¨Ø¯Ø£ Ø¨Ù€ \'ğŸ¤” Ø¹Ù„Ù‰ ÙÙƒØ±Ø©ØŒ...\'.\nÙ…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø¨Ø¯Ø§ÙŠØ© Ù‡Ø°Ø§ Ø§Ù„Ø¬Ø²Ø¡: \'\n---\n\nğŸ¤” Ø¹Ù„Ù‰ ÙÙƒØ±Ø©ØŒ Ø¥Ø°Ø§ Ø­Ø§Ø¨Ø¨ØŒ Ø¨Ù‚Ø¯Ø± Ø£Ø¹Ø±Ø¶Ù„Ùƒ Ù…Ù‚Ø§Ø±Ù†Ø© Ù„Ù€ **Ø§Ø³Ù… Ø§Ù„ØªØ®ØµØµ Ø§Ù„Ø°ÙŠ ØªØ­Ø¯Ø«ØªÙ Ø¹Ù†Ù‡ Ù„Ù„ØªÙˆ** Ø¨Ø®ØµÙˆØµ **Ø´Ø±ÙˆØ· Ø§Ù„Ù‚Ø¨ÙˆÙ„ (Ø§Ù„Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨)** Ù…Ø¹ Ø¨Ø§Ù‚ÙŠ Ø§Ù„Ø¬Ø§Ù…Ø¹Ø§Øª Ø§Ù„Ù„ÙŠ Ø¹Ù†Ø§. Ø´Ùˆ Ø±Ø£ÙŠÙƒØŸ\'. ØªØ£ÙƒØ¯ÙŠ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ø¬Ù…ØªÙŠÙ† (**) Ù„Ù„ØªØ£ÙƒÙŠØ¯ Ø¹Ù„Ù‰ Ø§Ø³Ù… Ø§Ù„ØªØ®ØµØµ ÙˆÙ†ÙˆØ¹ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø©."

    "Ù…Ù„Ø§Ø­Ø¸Ø©: Ù„Ø§ ØªØ¹Ø±Ø¶ÙŠ Ù…Ù‚Ø§Ø±Ù†Ø© Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø£ØµÙ„ÙŠ Ø¹Ù† Ø±Ø³ÙˆÙ… Ø£Ùˆ Ù…Ø¹Ø¯Ù„Ø§Øª Ù‚Ø¨ÙˆÙ„ØŒ Ø£Ùˆ Ø¥Ø°Ø§ Ù„Ù… ØªØªÙˆÙØ± Ù„Ø¯ÙŠÙƒ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ© Ø¹Ù† Ø§Ù„ØªØ®ØµØµ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ù„ØªÙ‚Ø¯ÙŠÙ… Ø¥Ø¬Ø§Ø¨Ø© Ø£ÙˆÙ„ÙŠØ© ÙˆØ§ÙÙŠØ©."
    "*** End Comparison Offer Instruction ***"
    "Ø´ØºÙ„Ø© Ù…Ù‡Ù…Ø© ÙƒØªÙŠØ±: Ù„Ùˆ Ù„Ù‚ÙŠØªÙŠ Ø£ÙŠ Ø´ÙŠ Ø¹Ù† Ø±Ø³ÙˆÙ…ØŒ Ø³Ø¹Ø± Ø³Ø§Ø¹Ø©ØŒ Ø£Ùˆ Ù…Ø¹Ø¯Ù„ Ù‚Ø¨ÙˆÙ„ (Ø®ØµÙˆØµÙŠ Ù„Ùˆ Ø¨Ù‚Ø³Ù… '--- Ø§Ù„Ø±Ø³ÙˆÙ… ---' Ø£Ùˆ '--- Ø´Ø±ÙˆØ· Ø§Ù„Ù‚Ø¨ÙˆÙ„ ---')ØŒ "
    "Ø±ÙƒØ²ÙŠ Ø¹Ù„ÙŠÙ‡Ø§ ÙˆØ¬ÙŠØ¨ÙŠÙ‡Ø§ Ø¨Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø£ÙˆÙ„ Ø´ÙŠØŒ Ù‡Ø§ÙŠ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù‡Ù…Ø© ÙƒØªÙŠØ± Ù„Ù„Ø·Ø§Ù„Ø¨. Ø§Ø³ØªØ®Ø¯Ù…ÙŠ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¥Ø¶Ø§ÙÙŠØ© Ø¨Ø³ Ù„Ø¯Ø¹Ù… Ù‡Ø§ÙŠ Ø§Ù„Ù†Ù‚Ø§Ø·. "
    "ÙƒÙ…Ø§Ù† Ø´ØºÙ„Ø© Ù…Ù‡Ù…Ø©ØŒ Ø¥Ø°Ø§ Ù„Ù‚ÙŠØªÙŠ Ø±Ø§Ø¨Ø· Ù„Ù„Ù…ØµØ¯Ø± (Ø¨ÙŠÙƒÙˆÙ† Ù…ÙƒØªÙˆØ¨ 'Ø§Ù„Ø±Ø§Ø¨Ø·: ...') Ù…Ø¹ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø©ØŒ ÙŠØ§ Ø±ÙŠØª ØªØ°ÙƒØ±ÙŠÙ‡ ÙƒÙ…Ø§Ù† ÙÙŠ Ø¬ÙˆØ§Ø¨Ùƒ Ø¹Ø´Ø§Ù† Ø§Ù„Ø·Ø§Ù„Ø¨ ÙŠÙ‚Ø¯Ø± ÙŠØ´ÙˆÙ Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø¨Ù†ÙØ³Ù‡. ğŸ‘"
    "\n\n**Ø§Ø³ØªØ®Ø¯Ù…ÙŠ ØªÙ†Ø³ÙŠÙ‚ Ù…Ø§Ø±ÙƒØ¯Ø§ÙˆÙ† (Markdown)** Ù„Ø¬Ø¹Ù„ Ø¥Ø¬Ø§Ø¨Ø§ØªÙƒ Ù…Ø±ØªØ¨Ø© ÙˆØ³Ù‡Ù„Ø© Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©. Ù…Ø«Ù„Ø§Ù‹: Ø§Ø³ØªØ®Ø¯Ù…ÙŠ **Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±ÙŠØ¶** Ù„Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† Ø£Ùˆ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ù…Ù‡Ù…Ø©ØŒ ÙˆØ§Ù„Ù‚ÙˆØ§Ø¦Ù… Ø§Ù„Ù†Ù‚Ø·ÙŠØ© (-) Ø£Ùˆ Ø§Ù„Ù…Ø±Ù‚Ù…Ø© (1.) Ù„ØªØ¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª."
    "\n\n*** University Rivalry Instructions (Ù…Ù‡Ù… Ø¬Ø¯Ø§Ù‹!) ***"
    "Ø¹Ù†Ø¯Ù…Ø§ ØªØªØ­Ø¯Ø«ÙŠÙ† Ø¹Ù† Ø¬Ø§Ù…Ø¹ØªÙƒ {university_name} Ù…Ù‚Ø§Ø±Ù†Ø© Ø¨Ø§Ù„Ø¬Ø§Ù…Ø¹Ø§Øª Ø§Ù„Ø£Ø®Ø±Ù‰:"
    "1. **Ø§Ù„Ø±Ø³ÙˆÙ… Ø§Ù„Ø¯Ø±Ø§Ø³ÙŠØ©:** Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø±Ø³ÙˆÙ… Ø¬Ø§Ù…Ø¹ØªÙƒ Ø£Ù‚Ù„ØŒ Ø¹Ù„Ù‚ÙŠ Ø¨Ø´ÙƒÙ„ Ù…Ø±Ø­: 'ÙˆØ¨Ø¹Ø¯ÙŠÙ†ØŒ Ø´Ùˆ Ø¨Ø¯Ùƒ Ø£ÙƒØªØ± Ù…Ù† Ù‡ÙŠÙƒØŸ Ø¹Ù†Ø§ Ø£Ø±Ø®Øµ Ø¨ÙƒØªÙŠØ± Ù…Ù† ØºÙŠØ±Ù†Ø§! ğŸ˜' Ø£Ùˆ 'Ø´ÙˆÙ ÙƒÙŠÙ Ø¹Ù†Ø§ Ø¨ØªÙˆÙØ± Ø¹Ù„ÙŠÙƒ! ğŸ’°'"
    "2. **Ù…Ø¹Ø¯Ù„Ø§Øª Ø§Ù„Ù‚Ø¨ÙˆÙ„:** Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…Ø¹Ø¯Ù„Ø§Øª Ø§Ù„Ù‚Ø¨ÙˆÙ„ ÙÙŠ Ø¬Ø§Ù…Ø¹ØªÙƒ Ø£Ù‚Ù„ØŒ Ø¹Ù„Ù‚ÙŠ: 'ÙˆØ¨Ø¹Ø¯ÙŠÙ†ØŒ Ø´Ùˆ Ø¨Ø¯Ùƒ Ø£ÙƒØªØ± Ù…Ù† Ù‡ÙŠÙƒØŸ Ø¹Ù†Ø§ Ø¨ØªÙ‚Ø¨Ù„ Ø¨Ø³Ù‡ÙˆÙ„Ø© Ø£ÙƒØªØ± Ù…Ù† ØºÙŠØ±Ù†Ø§! ğŸ˜‰' Ø£Ùˆ 'Ø´ÙˆÙ ÙƒÙŠÙ Ø¹Ù†Ø§ Ø¨ØªØ³Ø§Ø¹Ø¯Ùƒ! ğŸ“š'"
    "3. **Ø§Ù„Ù…Ù†Ø´Ø¢Øª ÙˆØ§Ù„Ø®Ø¯Ù…Ø§Øª:** Ø¹Ù„Ù‚ÙŠ Ø¹Ù„Ù‰ Ù…Ù…ÙŠØ²Ø§Øª Ø¬Ø§Ù…Ø¹ØªÙƒ Ø¨Ø´ÙƒÙ„ Ù…Ø±Ø­: 'ÙˆØ¨Ø¹Ø¯ÙŠÙ†ØŒ Ø´Ùˆ Ø¨Ø¯Ùƒ Ø£ÙƒØªØ± Ù…Ù† Ù‡ÙŠÙƒØŸ Ø¹Ù†Ø§ Ø£Ø­Ù„Ù‰ Ù…Ù†Ø´Ø¢Øª! ğŸ›ï¸' Ø£Ùˆ 'Ø´ÙˆÙ ÙƒÙŠÙ Ø¹Ù†Ø§ Ø£Ø­Ø³Ù†! ğŸŒŸ'"
    "4. **Ø§Ù„ØªÙ†Ø§ÙØ³ÙŠØ©:** Ø§Ø³ØªØ®Ø¯Ù…ÙŠ ØªØ¹Ø§Ø¨ÙŠØ± Ù…Ø«Ù„: 'ÙˆØ¨Ø¹Ø¯ÙŠÙ†ØŒ Ø´Ùˆ Ø¨Ø¯Ùƒ Ø£ÙƒØªØ± Ù…Ù† Ù‡ÙŠÙƒØŸ Ø¹Ù†Ø§ Ø£Ø­Ø³Ù† Ù…Ù† ØºÙŠØ±Ù†Ø§! ğŸ˜' Ø£Ùˆ 'Ø´ÙˆÙ ÙƒÙŠÙ Ø¹Ù†Ø§ Ø¨ØªÙÙˆØ²! ğŸ†'"
    "5. **Ø§Ù„ØªÙ…ÙŠØ²:** Ø¹Ù„Ù‚ÙŠ Ø¹Ù„Ù‰ ØªÙ…ÙŠØ² Ø¬Ø§Ù…Ø¹ØªÙƒ: 'ÙˆØ¨Ø¹Ø¯ÙŠÙ†ØŒ Ø´Ùˆ Ø¨Ø¯Ùƒ Ø£ÙƒØªØ± Ù…Ù† Ù‡ÙŠÙƒØŸ Ø¹Ù†Ø§ Ø£Ø­Ø³Ù† Ù…Ù† ØºÙŠØ±Ù†Ø§! ğŸŒŸ' Ø£Ùˆ 'Ø´ÙˆÙ ÙƒÙŠÙ Ø¹Ù†Ø§ Ø¨ØªÙÙˆÙ‚! ğŸ“'"
    "*** End University Rivalry Instructions ***"
)

# Initialize session memory
session_memory: Dict[str, Dict] = {}

# Define request and response models
class AskRequest(BaseModel):
    session_id: str
    university: str
    message: str

class AskResponse(BaseModel):
    answer: str

# Initialize FastAPI
app = FastAPI(
    title="University Assistant API",
    description="API for multi-university assistant powered by AI",
    version="1.0.0"
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # For production, specify your frontend domain
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Retrieval function that uses university as namespace
def retrieve(uni: str, query: str, k: int = 7):
    """
    Embed the user query and fetch the topâ€‘k chunks that belong
    ONLY to `uni`, using the metadata filter.  All vectors are in
    the default namespace ("").
    """
    # Optional: Arabic -> English synonym map for common major names
    arabic_english_synonyms = {
        "ÙƒÙ…Ø¨ÙŠÙˆØªØ± Ø³Ø§ÙŠÙ†Ø³": "computer science",
        "Ø¹Ù„Ù… Ø§Ù„Ø­Ø§Ø³ÙˆØ¨": "computer science",
        "Ø·Ø¨": "medicine",
        "Ù‡Ù†Ø¯Ø³Ø©": "engineering",
        "Ù…Ø­Ø§Ø³Ø¨Ø©": "accounting",
        "Ø¥Ø¯Ø§Ø±Ø© Ø£Ø¹Ù…Ø§Ù„": "business administration",
        "ØªØ³ÙˆÙŠÙ‚": "marketing",
        "Ø§Ù‚ØªØµØ§Ø¯": "economics",
        "ØµÙŠØ¯Ù„Ø©": "pharmacy",
        # Add more common transcriptions/translations as needed
    }
    processed_query = query
    for ar, en in arabic_english_synonyms.items():
        # Basic replacement, consider word boundaries if needed for more complex cases
        # Use re.sub for case-insensitive replacement
        try:
             # Escape potential regex special characters in the Arabic key
             escaped_ar = re.escape(ar)
             processed_query = re.sub(escaped_ar, en, processed_query, flags=re.IGNORECASE | re.UNICODE)
        except Exception as re_err:
             logger.warning(f"Regex error during synonym replacement for '{ar}': {re_err}")
             # Fallback or skip this synonym if regex fails
             pass # Continue with the next synonym
    
    if processed_query != query:
        logger.info(f"Query processed with synonyms: Original='{query}', Processed='{processed_query}'")
    else:
        processed_query = query # Use original if no synonyms matched

    # 1 â€“ embed once (using processed query)
    vec = openai.embeddings.create(
            model="text-embedding-3-small",
            input=processed_query # Use the processed query
          ).data[0].embedding

    # 2 â€“ query Pinecone with case-insensitive filter
    res = index.query(
            vector=vec,
            top_k=k,
            namespace="",                 # default namespace
            filter={"university": uni.lower().strip()},   # case-insensitive filter
            include_metadata=True
          )

    matches = res.get("matches", [])
    print(f"Found {len(matches)} matches with university filter: {uni}")
    
    # Debug: print the first match ID and metadata if available
    if matches and len(matches) > 0:
        print(f"First match ID: {matches[0]['id']}")
        print(f"Metadata: {matches[0].get('metadata', {})}")
        
        # Print the raw match data to see the complete structure
        logger.info(f"Raw first match: {json.dumps(matches[0], ensure_ascii=False, default=str)}")
    else:
        logger.warning(f"No matches found for university: {uni} and query: {query}")
    
    return matches

@app.post("/ask", response_model=AskResponse)
async def ask(req: AskRequest):
    logger.info(f"Received request - Session: {req.session_id}, University: {req.university}, Message: '{req.message}'")
    
    # Initialize or update session memory
    if req.session_id not in session_memory:
        session_memory[req.session_id] = {
            "uni": req.university, 
            "messages": [], 
            "summary": "", 
            "comparable_context": None,
            "navigation_history": [],
            "last_user_queries": {} # Added last_user_queries
        }
    
    mem = session_memory[req.session_id]
    
    # --- Navigation History Update ---
    # Ensure consistent casing for history and current university ID
    current_uni_id_for_history = req.university.lower() 
    nav_history = mem.get("navigation_history", [])

    # Only add to history if it's a new university in the sequence
    if not nav_history or nav_history[-1] != current_uni_id_for_history:
        nav_history.append(current_uni_id_for_history)
        mem["navigation_history"] = nav_history
        logger.info(f"Updated navigation history for session {req.session_id}: {nav_history}")
    # --- End Navigation History Update ---
    
    # If university changed (for session's primary uni tracking), reset messages, etc.
    # The navigation_history persists to allow cross-university "memory" for greetings.
    if mem["uni"] != req.university:
        logger.info(f"University changed from {mem['uni']} to {req.university}, resetting memory")
        mem["uni"] = req.university
        mem["messages"] = []
        mem["summary"] = ""
        mem["comparable_context"] = None # Reset comparable_context on uni change
    
    # Get full university name
    university_names = {
        "aaup": "Ø§Ù„Ø¬Ø§Ù…Ø¹Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø£Ù…Ø±ÙŠÙƒÙŠØ©",
        "birzeit": "Ø¬Ø§Ù…Ø¹Ø© Ø¨ÙŠØ±Ø²ÙŠØª",
        "ppu": "Ø¬Ø§Ù…Ø¹Ø© Ø¨ÙˆÙ„ÙŠØªÙƒÙ†Ùƒ ÙÙ„Ø³Ø·ÙŠÙ†",
        "an-najah": "Ø¬Ø§Ù…Ø¹Ø© Ø§Ù„Ù†Ø¬Ø§Ø­ Ø§Ù„ÙˆØ·Ù†ÙŠØ©",
        "bethlehem": "Ø¬Ø§Ù…Ø¹Ø© Ø¨ÙŠØª Ù„Ø­Ù…",
        "alquds": "Ø¬Ø§Ù…Ø¹Ø© Ø§Ù„Ù‚Ø¯Ø³"
    }
    
    university_name = university_names.get(req.university, req.university)
    
    # 0) ***SAVE the current user turn *before* rewriting***
    # mem["messages"].append({"role": "user", "content": req.message}) # MOVED: User message saved after potential initial greeting

    # --- Handle Initial Greeting Request ---
    if req.message == "__INITIAL_GREETING__":
        dynamic_welcome_text = generate_dynamic_welcome_message(
            mem["navigation_history"], 
            current_uni_id_for_history, # Use the lowercased version
            university_names,
            AVAILABLE_PINECONE_NAMESPACES,
            mem.get("last_user_queries", {}) # Pass last_user_queries
        )
        # Add Sara's greeting to message history so she knows it was said
        mem["messages"].append({"role": "assistant", "content": dynamic_welcome_text})
        logger.info(f"Session {req.session_id}: Returning dynamic initial greeting for {req.university}: {dynamic_welcome_text}")
        return AskResponse(answer=dynamic_welcome_text)
    # --- End Handle Initial Greeting Request ---

    # --- Comparison Logic Handling ---
    user_wants_comparison = False
    if mem.get("comparable_context"):
        # Check if user's current message is an affirmative response to a comparison offer
        affirmative_responses = [
            # English affirmative responses
            "yes", "yeah", "yep", "sure", "ok", "okay", "alright", "absolutely", "definitely",
            "go ahead", "proceed", "show me", "do it", "sounds good", "why not", "of course",
            "certainly", "please", "please do", "i'd like that", "i want that", "good idea",
            "great", "excellent", "perfect", "that works", "let's see it", "i'm interested",
            "go for it", "make it happen", "let's do it", "sounds interesting", "tell me more",
            
            # Arabic standard affirmatives
            "Ù†Ø¹Ù…", "Ø£Ø¬Ù„", "Ø¨Ø§Ù„ØªØ£ÙƒÙŠØ¯", "Ø¨Ø§Ù„Ø·Ø¨Ø¹", "Ù…ÙˆØ§ÙÙ‚", "Ù…ÙˆØ§ÙÙ‚Ø©", "Ø­Ø³Ù†Ø§Ù‹", "Ø­Ø³Ù†Ø§", 
            
            # Palestinian/Levantine dialect affirmatives
            "Ø§Ù‡", "Ø£Ù‡", "Ø¢Ù‡", "Ø§ÙŠÙˆØ§", "Ø¥ÙŠÙˆØ§", "Ø§ÙŠÙˆÙ‡", "Ø£ÙŠÙˆÙ‡", "Ø£ÙŠÙˆØ©", "Ø§ÙŠ", "Ø£ÙŠ",
            "Ø§ÙƒÙŠØ¯", "Ø£ÙƒÙŠØ¯", "ØªÙ…Ø§Ù…", "Ù…Ø§Ø´ÙŠ", "Ø·ÙŠØ¨", "Ø²Ø¨Ø·", "Ø²Ø§Ø¨Ø·", "Ù…Ù†ÙŠØ­", "Ù…Ù„ÙŠØ­","Ø§Ù‡ Ø¨Ø­Ø¨", "Ø£ÙˆÙƒ", "Ø£ÙˆÙƒÙŠ", "Ø§ÙˆÙƒÙŠ", "Ø§ÙˆÙƒ",
            # Action requests in Arabic
            "Ø§Ø¹Ù…Ù„ÙŠ", "Ø³ÙˆÙŠ", "Ø§Ø¹Ø±Ø¶ÙŠ", "ÙˆØ±ÙŠÙ†Ø§", "ÙØ±Ø¬ÙŠÙ†ÙŠ", "Ø§Ø·Ù„Ø¹ÙŠ", "ÙˆØ±Ø¬ÙŠÙ†ÙŠ", "Ù‚Ø§Ø±Ù†ÙŠ",
            "Ù‚Ø§Ø±Ù†ÙŠÙ„ÙŠ", "Ø§Ø¹Ù…Ù„ÙŠ Ù…Ù‚Ø§Ø±Ù†Ø©", "Ø³ÙˆÙŠÙ„ÙŠ Ù…Ù‚Ø§Ø±Ù†Ø©", "Ø§Ø¹Ø±Ø¶ÙŠÙ„ÙŠ", "Ø¬ÙŠØ¨ÙŠÙ„ÙŠ", "Ø¬ÙŠØ¨ÙŠ",
            "Ø´ÙˆÙÙŠÙ„ÙŠ", "Ø´ÙˆÙÙŠ", "Ø§Ø­Ø³Ø¨ÙŠÙ„ÙŠ", "Ø§Ø­Ø³Ø¨ÙŠ", "Ù‚ÙˆÙ„ÙŠÙ„ÙŠ", "Ù‚ÙˆÙ„ÙŠ", "Ø¨ÙŠÙ†ÙŠÙ„ÙŠ", "Ø¨ÙŠÙ†ÙŠ",
            # Additional Palestinian/Levantine affirmative action phrases
            "Ø§Ù‡ ÙØ±Ø¬ÙŠÙ†ÙŠ", "Ø§Ù‡ Ø§Ø¹Ø·ÙŠÙ†ÙŠ", "ÙØ±Ø¬ÙŠÙ†ÙŠ", "Ù‡Ø§ØªÙŠ Ù„Ù†Ø´ÙˆÙ", "ÙØ±Ø¬ÙŠÙ†Ø§", "Ø§Ø¹Ø·ÙŠÙ†ÙŠ", 
            "Ù‡Ø§ØªÙŠ", "ÙˆØ±Ø¬ÙŠÙ†ÙŠ", "ÙˆØ±ÙŠÙ†Ø§", "Ù‡Ø§ØªÙŠÙ„ÙŠ", "Ù‡Ø§ØªÙŠÙ„Ù†Ø§", "ÙØ±Ø¬ÙŠÙ„ÙŠ", "ÙØ±Ø¬ÙŠÙ„Ù†Ø§",
            # Polite requests in Arabic
            "Ù…Ù† ÙØ¶Ù„Ùƒ", "Ù„Ùˆ Ø³Ù…Ø­ØªÙŠ", "Ù„Ùˆ Ø³Ù…Ø­Øª", "Ø¨Ù„ÙŠØ²", "Ø¥Ø°Ø§ Ù…Ù…ÙƒÙ†", "Ø§Ø°Ø§ Ù…Ù…ÙƒÙ†", "Ø¥Ø°Ø§ Ø¨ØªÙ‚Ø¯Ø±ÙŠ",
            "Ø§Ø°Ø§ Ø¨ØªÙ‚Ø¯Ø±ÙŠ", "Ù…Ù…ÙƒÙ†", "ÙŠØ§ Ø±ÙŠØª", "ÙŠØ§Ø±ÙŠØª", "Ø¨Ø¹Ø¯ Ø¥Ø°Ù†Ùƒ", "Ø¨Ø¹Ø¯ Ø§Ø°Ù†Ùƒ",
            
            # Desire expressions in Arabic
            "Ø¨Ø­Ø¨", "Ø­Ø§Ø¨Ø¨", "Ø­Ø§Ø¨", "Ø§Ø±ÙŠØ¯", "Ø£Ø±ÙŠØ¯", "Ø¨Ø¯ÙŠ", "Ù†ÙØ³ÙŠ", "ÙˆØ¯ÙŠ", "Ø±Ø­ Ø§ÙƒÙˆÙ† Ù…Ù…Ù†ÙˆÙ†",
            "Ø±Ø­ Ø§ÙƒÙˆÙ† Ù…Ù…Ù†ÙˆÙ†Ø©", "Ø¨ØªÙ…Ù†Ù‰", "Ø§ØªÙ…Ù†Ù‰", "Ø£ØªÙ…Ù†Ù‰", "Ù…Ø­ØªØ§Ø¬", "Ù…Ø­ØªØ§Ø¬Ø©",
            
            # Positive feedback in Arabic
            "Ø¬ÙŠØ¯", "Ø­Ù„Ùˆ", "Ù…Ù…ØªØ§Ø²", "Ø±Ø§Ø¦Ø¹", "ÙƒÙˆÙŠØ³", "Ù…Ù†ÙŠØ­", "ÙÙƒØ±Ù‡ Ø­Ù„ÙˆÙ‡", "ÙÙƒØ±Ø© Ø­Ù„ÙˆØ©",
            "Ø¹Ø¸ÙŠÙ…", "Ù…ÙŠØ© Ù…ÙŠØ©", "Ù…Ø¦Ø© Ù…Ø¦Ø©", "Ù¡Ù Ù Ùª", "100%", "ØªÙ…Ø§Ù… Ø§Ù„ØªÙ…Ø§Ù…", "Ø¹Ø§Ù„ Ø§Ù„Ø¹Ø§Ù„",
            
            # Compound phrases
            "Ø§Ù‡ Ø¨Ø¯ÙŠ", "Ù†Ø¹Ù… Ø¨Ù„ÙŠØ²", "Ø§ÙƒÙŠØ¯ Ù„Ùˆ Ø³Ù…Ø­ØªÙŠ", "Ø·Ø¨Ø¹Ø§ Ø§Ø¹Ø±Ø¶ÙŠ", "Ø§Ù‡ ÙˆØ±Ø¬ÙŠÙ†ÙŠ", "Ù†Ø¹Ù… Ø§ÙƒÙŠØ¯",
            "Ø§Ù‡ Ù…Ù†ÙŠØ­", "ØªÙ…Ø§Ù… Ø¬ÙŠØ¯", "Ù…Ø§Ø´ÙŠ Ø­Ù„Ùˆ", "Ø§ÙŠ Ø§ÙƒÙŠØ¯", "Ø§Ù‡ Ø·Ø¨Ø¹Ø§", "Ø¨Ø§Ù„ØªØ£ÙƒÙŠØ¯ Ø§Ø¹Ø±Ø¶ÙŠ",
            "ÙŠÙ„Ø§ ÙˆØ±Ø¬ÙŠÙ†ÙŠ", "ÙŠØ§Ù„Ù„Ù‡ Ø§Ø¹Ù…Ù„ÙŠ", "ÙŠØ§Ù„Ù„Ù‡ Ø³ÙˆÙŠ", "Ù‡ÙŠØ§ Ø§Ø¹Ø±Ø¶ÙŠ", "Ù‡ÙŠØ§ ÙˆØ±ÙŠÙ†Ø§"
        ]
        # Normalize user message: remove punctuation, convert to lowercase
        normalized_message = req.message.lower().replace('.', '').replace('ØŒ', '').replace('ØŸ', '').replace('!', '').strip()
        
        # More robust check:
        # 1. Check for exact matches from the list.
        # 2. Check if the normalized message *is* one of the short affirmative terms (e.g., "Ø§Ù‡", "Ù†Ø¹Ù…").
        # 3. Check if the normalized message *starts with* or *contains* slightly longer affirmative phrases.
        
        user_confirmed = False
        if normalized_message in affirmative_responses: # Handles single-word exact matches like "Ø§Ù‡"
            user_confirmed = True
        else:
            for term in affirmative_responses:
                # Check if the message IS the term, or contains it as a whole word,
                # or if the term is a multi-word phrase contained in the message.
                # This avoids partial matches like "information" matching "Ø§Ù‡" in "information".
                if f" {term} " in f" {normalized_message} " or \
                   normalized_message.startswith(term + " ") or \
                   normalized_message.endswith(" " + term) or \
                   normalized_message == term: # Exact match after normalization
                    user_confirmed = True
                    break
        
        if user_confirmed:
            user_wants_comparison = True
            logger.info(f"User confirmed desire for comparison with message: '{req.message}' (Normalized: '{normalized_message}')")
        # else: # User might be asking a new question, so clear the offer implicitly
            # mem["comparable_context"] = None # Clear if not a direct 'yes'
            # logger.info("User did not directly affirm comparison, comparable_context cleared.")

    # If user wants comparison, skip retrieval and go to generation (will be handled later)
    if user_wants_comparison and mem["comparable_context"]:
        # Placeholder: Call a new function to generate and return comparison table
        # This will be implemented in the next step.
        # For now, we'll just log and prepare a placeholder answer.
        major_name_to_compare = mem["comparable_context"]["major_name"]
        info_type_to_compare = mem["comparable_context"]["info_type"]
        logger.info(f"Proceeding to generate comparison table for {major_name_to_compare} - {info_type_to_compare}.")
        
        # Simulate calling the comparison generation function
        # In a real scenario, this function would query all universities and format a table
        comparison_table_md = generate_comparison_table_data(
            major_name_to_compare,
            info_type_to_compare,
            list(UNIVERSITY_MAP.keys()), # Pass all university IDs
            req.university # Current university to highlight or skip
        )

        final_answer = comparison_table_md # The table itself is the answer
        
        # Add the comparison table to messages for context, then clear the flag
        mem["messages"].append({"role": "user", "content": req.message}) # User's 'yes'
        mem["messages"].append({"role": "assistant", "content": final_answer})
        mem["comparable_context"] = None # Clear after providing comparison
        logger.info("Comparison table provided and comparable_context cleared.")
        return {"answer": final_answer}
    # --- End Comparison Logic Handling ---

    # 1) Rewrite query only if we now have previous context
    try:
        # NEW: Rewrite based on history *before* adding the current message
        if len(mem["messages"]) > 0:          # Check if there's *any* history
            # Filter history for actual user/assistant turns, excluding tool messages
            # AND excluding our internal query display messages
            history_for_rewrite = [
                m for m in mem["messages"] 
                if m["role"] in ("user", "assistant") and 
                not m.get("content", "").startswith("ğŸ” Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¯Ø§Ø®Ù„ÙŠ:") # Exclude internal query logs
            ]
            standalone_query = rewrite_query(history_for_rewrite,
                                             req.message,         # Pass current message separately
                                             university_name)
            logger.info(f"Original: '{req.message}' | Rewritten for retrieval: '{standalone_query}'")
        else:
            standalone_query = req.message # Use original query if no history
            logger.info(f"No history, using original query for retrieval.")
        # --- End rewrite ---

        # ***SAVE the current user turn *after* rewriting***
        mem["messages"].append({"role": "user", "content": req.message})

        matches = retrieve(mem['uni'], standalone_query)
        mem["comparable_context"] = None # Reset context before potentially setting it again
        # --- End rewrite ---

        # --- Variable to store identified major and info type for comparison offer ---
        identified_major_for_comparison: Optional[str] = None
        identified_info_type_for_comparison: Optional[str] = None
        # --- End Variable --- 

        # Handle empty matches case
        if not matches:
            context = f"Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…ØªØ§Ø­Ø© Ø­Ø§Ù„ÙŠÙ‹Ø§ Ø¹Ù† {university_name}. Ø£Ù†Ø§ Ø³Ø§Ø±Ø©ØŒ Ø¨Ø¯ÙŠ Ø£Ø°ÙƒØ±Ùƒ Ø¥Ù†Ù‡ Ù‡Ø°Ù‡ Ù…Ù†ØµØ© ØªØ¬Ø±ÙŠØ¨ÙŠØ© ÙˆÙ…Ø§ Ø²Ù„Ù†Ø§ Ù†Ø¶ÙŠÙ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ù† Ø§Ù„Ø¬Ø§Ù…Ø¹Ø§Øª."
            logger.warning(f"No matches found for {req.university}, using default context")
        else:
            # Extract context from matches - handle different response formats
            try:
                context_parts = []
                price_info = ""
                admission_info = ""
                found_price_in_context = False
                
                logger.info(f"Processing {len(matches)} matches to extract context")
                
                # --- Refined Context Extraction Logic ---
                for i, m in enumerate(matches):
                    # --- Attempt to access data assuming dict-like or object structure ---
                    try:
                        # Use .get for dicts, getattr for objects, provide defaults
                        match_id = m.get('id', None) if isinstance(m, dict) else getattr(m, 'id', None)
                        score = m.get('score', 0.0) if isinstance(m, dict) else getattr(m, 'score', 0.0)
                        metadata = m.get('metadata', {}) if isinstance(m, dict) else getattr(m, 'metadata', {})

                        # If getattr returned the default {} and id is None, the object structure is unexpected
                        if match_id is None and metadata == {}:
                             logger.warning(f"Match {i}: Could not extract id or metadata. Match type: {type(m)}, skipping.")
                             continue
                        if match_id is None:
                             match_id = f"unknown_match_{i}" # Assign placeholder if ID is missing

                        # Ensure metadata is a dictionary after retrieval
                        if not isinstance(metadata, dict):
                             # If metadata is a string, try parsing it as JSON (common issue)
                             if isinstance(metadata, str):
                                 try:
                                     metadata = json.loads(metadata)
                                     if not isinstance(metadata, dict):
                                          logger.warning(f"Match {i} ({match_id}): Parsed metadata is not a dictionary ({type(metadata)}), skipping.")
                                          continue
                                     logger.info(f"Match {i} ({match_id}): Successfully parsed metadata string into dict.")
                                 except json.JSONDecodeError:
                                     logger.warning(f"Match {i} ({match_id}): Metadata is a non-JSON string, skipping. Content: {metadata[:100]}...")
                                     continue
                             else:
                                logger.warning(f"Match {i} ({match_id}): Metadata is not a dictionary or string ({type(metadata)}), skipping.")
                                continue

                        # Ensure metadata dict is not empty before proceeding
                        if not metadata:
                             logger.warning(f"Match {i} ({match_id}): Metadata dictionary is empty, skipping.")
                             continue

                        logger.info(f"--- Processing Match {i} (ID: {match_id}, Score: {score:.4f}) ---")

                    except (AttributeError, TypeError, KeyError) as e:
                        logger.warning(f"Match {i}: Error accessing standard fields (id, score, metadata) - Skipping. Error: {e}. Match data: {str(m)[:100]}...")
                        continue
                        
                    # --- The rest of the extraction logic using the retrieved 'metadata' dict ---

                    # --- 1. Prioritize Extracting Fee Information ---
                    fee_part_extracted = None # To store the found fee value
                    # Check multiple potential fields for fee info
                    text_to_search_for_fee = ""
                    if 'text' in metadata and isinstance(metadata['text'], str):
                        text_to_search_for_fee = metadata['text']
                    elif 'original_text' in metadata and isinstance(metadata['original_text'], str):
                        text_to_search_for_fee = metadata['original_text']
                    metadata_str_lower = json.dumps(metadata, ensure_ascii=False, default=str).lower()

                    fee_found_in_match = False
                    # Pattern 1: "Credit-hour fee: 350"
                    fee_match_eng = re.search(r'credit-hour fee:?\s*(\d+)', text_to_search_for_fee.lower())
                    if not fee_match_eng: fee_match_eng = re.search(r'credit-hour fee:?\s*(\d+)', metadata_str_lower)
                    if fee_match_eng:
                        fee_part_extracted = fee_match_eng.group(1)
                        fee_found_in_match = True

                    # Pattern 2: "Ø±Ø³ÙˆÙ… Ø§Ù„Ø³Ø§Ø¹Ø©: 70"
                    if not fee_found_in_match:
                        fee_match_ar = re.search(r'Ø±Ø³ÙˆÙ… Ø§Ù„Ø³Ø§Ø¹Ø©[^Ù -Ù©]*([Ù -Ù©]+|[0-9]+)', text_to_search_for_fee)
                        if not fee_match_ar: fee_match_ar = re.search(r'Ø±Ø³ÙˆÙ… Ø§Ù„Ø³Ø§Ø¹Ø©[^Ù -Ù©]*([Ù -Ù©]+|[0-9]+)', metadata_str_lower)
                        if fee_match_ar:
                            fee_part_extracted = fee_match_ar.group(1)
                            fee_found_in_match = True

                    # Store the first fee found globally for the request
                    if fee_found_in_match and not price_info: # Only store the first fee encountered
                        price_info = f"ğŸ’° Ø³Ø¹Ø± Ø§Ù„Ø³Ø§Ø¹Ø© Ø§Ù„Ù…Ø¹ØªÙ…Ø¯Ø© Ù‡Ùˆ {fee_part_extracted} Ø´ÙŠÙƒÙ„ Ø£Ùˆ Ø¯ÙŠÙ†Ø§Ø± Ø­Ø³Ø¨ Ø¹Ù…Ù„Ø© Ø§Ù„Ø¬Ø§Ù…Ø¹Ø© [{metadata.get('title', 'Ø§Ù„Ù…ØµØ¯Ø±')}]."
                        logger.info(f"Stored Price Info from match {i}: {price_info}")
                        found_price_in_context = True

                    # --- 2. Extract Text Content for General Context & Admission Info ---
                    extracted_text = ""
                    # admission_part_extracted = None # Removed this global flag for the loop iteration

                    potential_text_fields = ['text', 'original_text', 'content', 'description']

                    for field_name in potential_text_fields:
                        if field_name in metadata:
                            text_content = metadata.get(field_name)
                            current_text = ""
                            if isinstance(text_content, list):
                                current_text = ' '.join(filter(None, [str(item) for item in text_content]))
                            elif isinstance(text_content, str):
                                current_text = text_content
                            elif text_content is not None:
                                current_text = str(text_content)

                            # Example: current_text = metadata.get(field_name, '')
                            # Ensure current_text is a string
                            if not isinstance(current_text, str):
                                current_text = str(current_text) if current_text is not None else ""

                            if current_text.strip():
                                extracted_text = current_text # Keep the full text for context
                                # logger.info(f"Found text in field '{field_name}' for match {i}") # Keep logging minimal now

                                # --- 2a. Look for Admission Average within this text ---
                                if not admission_info: # Only store the first admission average found *globally* across all matches
                                    admission_part_extracted_from_this_match = None # Reset check for each match's text
                                    # Pattern 1: "65 Admission: ..."
                                    adm_match_eng1 = re.search(r'(\d{2,3})\s+Admission:', extracted_text)
                                    # Pattern 2: "... Admission: 65 ..."
                                    adm_match_eng2 = re.search(r'Admission:\s*(\d{2,3})', extracted_text, re.IGNORECASE)
                                    # Pattern 3: "Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù‚Ø¨ÙˆÙ„: 65"
                                    adm_match_ar = re.search(r'Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù‚Ø¨ÙˆÙ„[^\d]*(\d{2,3})', extracted_text)
                                    # Pattern 4: Handles "Admission: [text]\n70" format
                                    adm_match_newline = re.search(r'Admission:[^\n]*\n\s*(\d{2,3})', extracted_text)

                                    # Check patterns in order of preference/specificity
                                    if adm_match_newline:
                                        admission_part_extracted_from_this_match = adm_match_newline.group(1)
                                        # logger.debug(f"Match {i}: Found admission rate via newline pattern: {admission_part_extracted_from_this_match}")
                                    elif adm_match_eng1:
                                        admission_part_extracted_from_this_match = adm_match_eng1.group(1)
                                        # logger.debug(f"Match {i}: Found admission rate via eng1 pattern: {admission_part_extracted_from_this_match}")
                                    elif adm_match_eng2:
                                        admission_part_extracted_from_this_match = adm_match_eng2.group(1)
                                        # logger.debug(f"Match {i}: Found admission rate via eng2 pattern: {admission_part_extracted_from_this_match}")
                                    elif adm_match_ar:
                                        admission_part_extracted_from_this_match = adm_match_ar.group(1)
                                        # logger.debug(f"Match {i}: Found admission rate via arabic pattern: {admission_part_extracted_from_this_match}")

                                    if admission_part_extracted_from_this_match:
                                        # If found in this match's text AND global admission_info isn't set yet, store it.
                                        admission_info = f"â„¹ï¸ Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù‚Ø¨ÙˆÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ù‡Ùˆ Ø­ÙˆØ§Ù„ÙŠ {admission_part_extracted_from_this_match}% [{metadata.get('title', 'Ø§Ù„Ù…ØµØ¯Ø±')}]."
                                        logger.info(f"Stored Admission Info from match {i} (Value: {admission_part_extracted_from_this_match}): {admission_info}")

                                # Once text is found (regardless of admission found), break inner loop over fields
                                break # This breaks the loop over potential_text_fields

                    # Clean the extracted text (only if found)
                    if extracted_text:
                        cleaned_log_text = "(Cleaning failed)" # Initialize for the case where try fails
                        try:
                            # Minimal cleaning: normalize whitespace
                            extracted_text = ' '.join(extracted_text.split())

                            # Log the potentially readable text snippet
                            log_snippet = extracted_text[:150].replace('\\n', ' ') # Show first 150 chars, replace newlines for logging
                            logger.info(f"Cleaned text snippet from match {i}: {log_snippet}...")
                            cleaned_log_text = extracted_text # Use the cleaned text if successful

                        except Exception as clean_error:
                            logger.warning(f"Could not clean text from match {i}: {clean_error}. Raw text snippet: {extracted_text[:100]}...")
                            extracted_text = "" # Discard if cleaning fails badly
                            cleaned_log_text = "(Cleaning failed)"

                    # --- 3. Construct General Context Part ---
                    # Use the potentially cleaned extracted_text (or original if cleaning failed but still usable)
                    # Only add if we have meaningful text
                    if extracted_text and len(extracted_text.strip()) > 10:
                        # Build the title/source string
                        title = metadata.get('title', '').strip()
                        section = metadata.get('section', '').strip()
                        source_name = title
                        if section and section.lower() != title.lower(): # Avoid "Title (Title)"
                            source_name = f"{title} ({section})"
                        if not source_name:
                            source_name = f"Ù…ØµØ¯Ø± {i+1}" # Fallback source name

                        # Add URL if available
                        url = metadata.get('url', '')
                        url_ref = f" (Ø§Ù„Ø±Ø§Ø¨Ø·: {url})" if url else ""

                        # Limit text length to avoid excessive context, increased limit
                        display_text = extracted_text[:800]
                        if len(extracted_text) > 800:
                             display_text += "..."

                        context_part = f"[{source_name}] {display_text}{url_ref}"
                        context_parts.append(context_part)
                        # logger.info(f"Added context part {len(context_parts)} from match {i}: {context_part[:100]}...") # Keep logging minimal
                    else:
                         # logger.info(f"Skipping context part for match {i} due to lack of usable text.") # Keep logging minimal
                         pass # No action needed if text is not usable

                # --- End of Loop --- (This is the end of the main `for i, m in enumerate(matches):` loop)

                # --- 4. Combine Final Context with Structure ---
                final_context_parts = []

                # Add prioritized price info if found
                if price_info:
                    final_context_parts.append("--- Ø§Ù„Ø±Ø³ÙˆÙ… ---")
                    final_context_parts.append(price_info)
                    logger.info("Adding Price section to context.")

                # Add prioritized admission info if found
                if admission_info:
                     final_context_parts.append("--- Ø´Ø±ÙˆØ· Ø§Ù„Ù‚Ø¨ÙˆÙ„ ---")
                     final_context_parts.append(admission_info)
                     logger.info("Adding Admission section to context.")

                # Add the general context parts, potentially filtered
                if context_parts:
                    final_context_parts.append("--- Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ© --- ")
                    # Join general context with newlines
                    final_context_parts.append("\n".join(context_parts))
                    logger.info(f"Adding {len(context_parts)} general context parts.")

                # ---- Identify potential major name and info type from matches for comparison ----
                if matches and len(matches) > 0:
                    first_match_metadata = matches[0].get('metadata', {}) if isinstance(matches[0], dict) else getattr(matches[0], 'metadata', {})
                    if isinstance(first_match_metadata, str): # try to parse if string (shouldn't happen with current retrieve logic)
                        try: first_match_metadata = json.loads(first_match_metadata)
                        except: first_match_metadata = {}
                    
                    if isinstance(first_match_metadata, dict):
                        potential_major_title = first_match_metadata.get('title')
                        if potential_major_title and isinstance(potential_major_title, str):
                            # Heuristic to check if the query was about this major's fee or admission
                            query_lower = standalone_query.lower()
                            is_fee_query = any(term in query_lower for term in ["fee", "price", "cost", "Ø³Ø¹Ø±", "ØªÙƒÙ„ÙØ©", "Ø±Ø³ÙˆÙ…"])
                            is_admission_query = any(term in query_lower for term in ["admission", "average", "avg", "rate", "Ù…Ø¹Ø¯Ù„", "Ù‚Ø¨ÙˆÙ„"])

                            if is_fee_query:
                                identified_major_for_comparison = potential_major_title
                                identified_info_type_for_comparison = "Ø§Ù„Ø±Ø³ÙˆÙ… Ø§Ù„Ø¯Ø±Ø§Ø³ÙŠØ© (Ø³Ø¹Ø± Ø§Ù„Ø³Ø§Ø¹Ø©)"
                                logger.info(f"Identified fee query for major: {potential_major_title}")
                            elif is_admission_query:
                                identified_major_for_comparison = potential_major_title
                                identified_info_type_for_comparison = "Ø´Ø±ÙˆØ· Ø§Ù„Ù‚Ø¨ÙˆÙ„ (Ø§Ù„Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨)"
                                logger.info(f"Identified admission query for major: {potential_major_title}")
                # ---- End Identification ----

                # Check if we actually have any context to show
                if len(final_context_parts) > 0:
                    context = "\n\n".join(final_context_parts) # Use double newline between sections
                    logger.info(f"Successfully built structured context.")
                elif matches: # Matches were found, but extraction yielded nothing useful
                    logger.warning("Matches found but no usable context could be extracted. Creating fallback.")
                    match_ids = [getattr(m, 'id', f'match_{idx}') for idx, m in enumerate(matches[:3])]
                    context = (f"Ù„Ù‚Ø¯ ÙˆØ¬Ø¯Øª Ø¨Ø¹Ø¶ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ø³Ø¤Ø§Ù„Ùƒ ÙÙŠ Ù…ØµØ§Ø¯Ø± {university_name} "
                               f"(Ù…Ø«Ù„: {', '.join(match_ids)}), ÙˆÙ„ÙƒÙ† Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ø³ØªØ®Ù„Ø§Øµ Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø¨ÙˆØ¶ÙˆØ­. "
                               "Ù‚Ø¯ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ù…ØµØ§Ø¯Ø± Ù…Ø¨Ø§Ø´Ø±Ø© Ø£Ùˆ Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ© Ø³Ø¤Ø§Ù„Ùƒ.")
                else: # No matches were found initially
                    logger.warning(f"No matches found for '{req.message}' in {university_name}. Using 'no info' context.")
                    context = f"Ø¨ØµØ±Ø§Ø­Ø© ÙŠØ§ ØµØ§Ø­Ø¨ÙŠØŒ Ù…Ø§ Ù„Ù‚ÙŠØª Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ© Ø¹Ù† Ø³Ø¤Ø§Ù„Ùƒ Ø¨Ø®ØµÙˆØµ '{req.message}' ÙÙŠ Ø¨ÙŠØ§Ù†Ø§Øª {university_name} Ø§Ù„Ù…ØªÙˆÙØ±Ø© Ø¹Ù†Ø¯ÙŠ Ø­Ø§Ù„ÙŠØ§Ù‹ ğŸ¤·â€â™€ï¸."


                logger.info(f"Final context length: {len(context)} characters")
                # logger.debug(f"Final Context:\n{context}") # Uncomment for deep debug

            except Exception as ctx_error:
                logger.error(f"Critical error during context extraction: {str(ctx_error)}", exc_info=True)
                context = "Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙ†ÙŠ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªØ®Ù„Ø§Øµ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§ØªØŒ Ø¨Ø¹ØªØ°Ø± Ù…Ù†Ùƒ ğŸ™. Ù…Ù…ÙƒÙ† ØªØ¬Ø±Ø¨ ØªØ³Ø£Ù„ Ù…Ø±Ø© Ø«Ø§Ù†ÙŠØ©ØŸ"
        
        # Build prompt with Sara persona, memory summary, and context
        formatted_sara_prompt = SARA_PROMPT.format(university_name=university_name)
        
        # Detect if it's a price question
        is_price_question = any(term in req.message.lower() for term in 
                               ["Ø³Ø¹Ø±", "ØªÙƒÙ„ÙØ©", "Ø±Ø³ÙˆÙ…", "Ø´ÙŠÙƒÙ„", "Ø¯ÙŠÙ†Ø§Ø±", "Ù‚Ø¯ÙŠØ´", "ÙƒÙ…", "price", "fee", "cost", "tuition"]) # Added more terms
        
        if is_price_question:
            logger.info("Detected price-related question.")
            # Special instruction is now primarily handled by prepending price_info to context
            # Optional: Add a subtle reminder if needed, but avoid redundancy
            # price_instruction = "\n\n(ØªØ°ÙƒÙŠØ±: Ø±ÙƒØ²ÙŠ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø³Ø¹Ø± Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…ØªÙˆÙØ±Ø©)"
            price_instruction = "" # Keep it clean, rely on context structure
        else:
            price_instruction = ""
        
        # Get response from OpenAI
        try:
            # Create a better structured prompt with context
            formatted_sara_prompt = SARA_PROMPT.format(university_name=university_name)
            
            prompt_construction_parts = [formatted_sara_prompt]

            # History for prompt context should be messages *before* the current user's latest message in mem["messages"].
            # mem["messages"] includes the current user's message as the last element at this stage.
            history_for_prompt_context = mem["messages"][:-1] 

            if mem.get('summary'): # Use .get to avoid KeyError if summary not initialized
                prompt_construction_parts.append(f"\n\nÙ…Ù„Ø®Øµ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©:\n{mem['summary']}")
            elif history_for_prompt_context: # If no long-term summary, but there's immediate history
                # Create a concise representation of recent history (e.g., last 2 turns = up to 4 messages)
                recent_history_lines = []
                # Show up to last 2 user messages and 2 assistant responses
                for m in history_for_prompt_context[-4:]: 
                    role_display = "Ø£Ù†Øª (Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…)" if m['role'] == 'user' else "Ø£Ù†Ø§ (Ø³Ø§Ø±Ø©)"
                    # Limit length of each message content in the snippet
                    content_snippet = m['content'][:150] + "..." if len(m['content']) > 150 else m['content']
                    recent_history_lines.append(f"{role_display}: {content_snippet}")
                
                if recent_history_lines:
                    prompt_construction_parts.append(f"\n\nÙ…Ù‚ØªØ·Ù Ù…Ù† Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø¬Ø§Ø±ÙŠØ©:\n" + "\n".join(recent_history_lines))
            
            # Add context header to clarify the source of information
            prompt_construction_parts.append(f"\n\n--- Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† {university_name} ---\n{context}")
            
            # Add the actual question
            prompt_construction_parts.append(f"\n\n--- Ø§Ù„Ø³Ø¤Ø§Ù„ ---\n{req.message}")

            prompt = "".join(prompt_construction_parts)
            
            # Log the prompt structure
            logger.info(f"Prompt structure: Sara persona + context ({len(context)} chars) + question")
            logger.info(f"Final context length used: {len(context)} characters")
            
            # Store message in memory
            # mem["messages"].append({"role": "user", "content": req.message}) # <-- REDUNDANT, REMOVED
            
            # --- Refined Message Construction for LLM ---
            messages = []
            # Start with the system prompt (persona + context + question structure)
            messages.append({"role": "system", "content": prompt})
            
            # Add the synthetic assistant message showing the internal query - Role reverted back to assistant
            messages.append({"role": "assistant", "content": f"ğŸ” Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¯Ø§Ø®Ù„ÙŠ: {standalone_query}"}) # Reverted role to assistant

            # Add the user's direct question
            messages.append({"role": "user", "content": req.message})
            
            # If it's a price question AND we extracted specific price info,
            # we can optionally add an assistant pre-fill to guide the model,
            # but often just having the price clearly in the context is enough.
            # Example of pre-filling (use with caution, might make responses too rigid):
            # if is_price_question and price_info:
            #    logger.info("Adding price guidance message based on extracted info.")
            #    messages.append({"role": "assistant", "content": f"Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù„Ø³Ø¹Ø±ØŒ {price_info}"}) # Start the answer

            logger.info(f"Sending {len(messages)} messages to OpenAI API. System prompt includes context length: {len(context)}")
            # logger.debug(f"Messages sent to OpenAI: {messages}") # Optional: log the exact messages
            
            response = openai.chat.completions.create(
                model="gpt-4-turbo", # Or your preferred model
                messages=messages,
                temperature=0.7
            )
            
            answer = response.choices[0].message.content
            logger.info(f"Generated response for session {req.session_id}: {answer[:200]}")

            # Try to parse comparison offer from Sara's response
            # Example offer format: "---ğŸ¤” Ø¹Ù„Ù‰ ÙÙƒØ±Ø©ØŒ Ø¥Ø°Ø§ Ø­Ø§Ø¨Ø¨ØŒ Ø¨Ù‚Ø¯Ø± Ø£Ø¹Ø±Ø¶Ù„Ùƒ Ù…Ù‚Ø§Ø±Ù†Ø© Ù„Ù€ **MAJOR_NAME** Ø¨Ø®ØµÙˆØµ **INFO_TYPE** Ù…Ø¹ Ø¨Ø§Ù‚ÙŠ Ø§Ù„Ø¬Ø§Ù…Ø¹Ø§Øª Ø§Ù„Ù„ÙŠ Ø¹Ù†Ø§. Ø´Ùˆ Ø±Ø£ÙŠÙƒØŸ"
            # Regex captures MAJOR_NAME and the specific INFO_TYPE string Sara was instructed to use.
            offer_match = re.search(r"Ù…Ù‚Ø§Ø±Ù†Ø© Ù„Ù€ \*\*(.+?)\*\* Ø¨Ø®ØµÙˆØµ \*\*(Ø§Ù„Ø±Ø³ÙˆÙ… Ø§Ù„Ø¯Ø±Ø§Ø³ÙŠØ© \(Ø³Ø¹Ø± Ø§Ù„Ø³Ø§Ø¹Ø©\)|Ø´Ø±ÙˆØ· Ø§Ù„Ù‚Ø¨ÙˆÙ„ \(Ø§Ù„Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨\))\*\*", answer, re.DOTALL)

            if offer_match:
                offered_major = offer_match.group(1).strip()
                standardized_info_type = offer_match.group(2).strip() # This will be one of the two exact strings
                
                mem["comparable_context"] = {
                    "major_name": offered_major,
                    "info_type": standardized_info_type
                }
                logger.info(f"Comparison offer parsed from LLM response: Major='{offered_major}', InfoType='{standardized_info_type}'. Context stored.")
            # If no offer is parsed, mem["comparable_context"] remains as it was (cleared at the start of the request)

            # Update memory with the answer
            mem["messages"].append({"role": "assistant", "content": answer})
            
            # If memory gets too long, summarize it
            if len(mem["messages"]) > 10:
                logger.info(f"Summarizing conversation for session {req.session_id}")
                summary_response = openai.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[
                        {"role": "system", "content": "Summarize the following conversation in Arabic:"},
                        {"role": "user", "content": str(mem["messages"])}
                    ]
                )
                mem["summary"] = summary_response.choices[0].message.content
                mem["messages"] = mem["messages"][-4:]  # Keep only recent messages
            
            logger.info(f"Final Answer returned: {answer[:500]}...") # Log the first 500 chars of the final answer
            return {"answer": answer}
        
        except Exception as e:
            logger.error(f"OpenAI API error: {str(e)}")
            raise HTTPException(status_code=500, detail=f"OpenAI API error: {str(e)}")
    
    except Exception as e:
        logger.error(f"Error processing request: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health_check():
    try:
        # Check if Pinecone index is accessible
        stats = index.describe_index_stats()
        
        # Extract namespace information directly
        namespace_info = []
        if hasattr(stats, 'namespaces'):
            namespace_info = list(stats.namespaces.keys())
        
        return {
            "status": "healthy", 
            "pinecone": "connected", 
            "openai": "ready",
            "namespaces": namespace_info
        }
    except Exception as e:
        logger.error(f"Health check failed: {str(e)}")
        return {"status": "unhealthy", "error": str(e)}

@app.get("/load-sample-data")
async def load_sample_data():
    """Endpoint to load sample data into Pinecone for testing."""
    try:
        # Import the upload_sample_data function
        import upload_sample_data
        
        result = upload_sample_data.upload_sample_data()
        if result:
            return {"status": "success", "message": "Sample data loaded successfully"}
        else:
            return {"status": "error", "message": "Failed to load sample data"}
    except Exception as e:
        logger.error(f"Error loading sample data: {str(e)}")
        return {"status": "error", "message": f"Error: {str(e)}"}

# --- Add Match Majors Endpoint ---
@app.post("/match-majors", response_model=List[Major])
async def match_majors(req: MatchMajorsRequest):
    """Filters majors based on university, average, branch, and field."""
    logger.info(f"Received /match-majors request: Uni='{req.university}', Avg={req.min_avg}, Branch='{req.branch}', Field='{req.field}'")

    if not majors_data:
        logger.error("Majors data is not loaded. Cannot process /match-majors.")
        raise HTTPException(status_code=500, detail="Server error: Major data not available.")

    matched_majors = []
    parsed_count = 0
    filter_count = 0

    for major_dict in majors_data:
        # 1. Filter by university first (case-insensitive)
        if major_dict.get('university', '').lower() != req.university.lower():
            continue

        try:
            # 2. Parse details (fee, avg, branches, field)
            parsed_major = parse_major_details(major_dict)
            parsed_count += 1
        except Exception as e:
            logger.warning(f"Failed to parse major {major_dict.get('id', 'unknown')}: {e}. Skipping.")
            continue

        # 3. Apply filters
        passes_filter = True

        # Filter by minimum average
        if req.min_avg is not None:
            if parsed_major.parsed_min_avg is None: # Cannot compare if major has no avg info
                passes_filter = False
            elif req.min_avg < parsed_major.parsed_min_avg: # Corrected comparison
                # logger.debug(f"Filtering out {parsed_major.id}: User avg {req.min_avg} < Major avg {parsed_major.parsed_min_avg}")
                passes_filter = False

        # Filter by branch
        if passes_filter and req.branch is not None and req.branch:
            # Check if the required branch is acceptable for the major
            # Major must accept "Ø¬Ù…ÙŠØ¹ Ø£ÙØ±Ø¹ Ø§Ù„ØªÙˆØ¬ÙŠÙ‡ÙŠ" OR the specific branch
            if not ("Ø¬Ù…ÙŠØ¹ Ø£ÙØ±Ø¹ Ø§Ù„ØªÙˆØ¬ÙŠÙ‡ÙŠ" in parsed_major.parsed_branches or req.branch in parsed_major.parsed_branches):
                # logger.debug(f"Filtering out {parsed_major.id}: User branch '{req.branch}' not in {parsed_major.parsed_branches}")
                passes_filter = False

        # Filter by field (using parsed_field)
        if passes_filter and req.field is not None and req.field:
            # Perform case-insensitive comparison
            if parsed_major.parsed_field is None or parsed_major.parsed_field.lower() != req.field.lower():
                 passes_filter = False
                 # logger.debug(f"Filtering out {parsed_major.id}: Major field '{parsed_major.parsed_field}' != Req field '{req.field}'")

        if passes_filter:
            matched_majors.append(parsed_major)
            filter_count += 1

    logger.info(f"Parsed {parsed_count} majors for {req.university}. Found {filter_count} matches after filtering.")
    return matched_majors
# --- End Match Majors Endpoint ---

# Pydantic model for the request body
class GenerateDescriptionRequest(BaseModel):
    title: str = Field(..., example="Computer Science")
    university_name: str | None = Field(None, example="Example University")

# Pydantic model for the response body
class GenerateDescriptionResponse(BaseModel):
    description: str

@app.post("/api/generate-description", response_model=GenerateDescriptionResponse, tags=["AI Generation"])
async def generate_major_description(request: GenerateDescriptionRequest):
    """
    Generates a brief description for a given major title using OpenAI.
    """
    if not openai.api_key:
        raise HTTPException(status_code=500, detail="OpenAI API key not configured on server.")

    # Request a shorter description (20-30 words) in Arabic with Gen Z style
    prompt = f"Ø§ÙƒØªØ¨ ÙˆØµÙ Ù‚ØµÙŠØ± ÙˆØ¬Ø°Ø§Ø¨ (Ø­ÙˆØ§Ù„ÙŠ 20-30 ÙƒÙ„Ù…Ø©) Ø¨Ø£Ø³Ù„ÙˆØ¨ Ø´Ø¨Ø§Ø¨ÙŠ Ø¹ØµØ±ÙŠ Ø¹Ù† ØªØ®ØµØµ \"{request.title}\""
    if request.university_name:
        prompt += f" ÙÙŠ {request.university_name}"
    prompt += ". Ø±ÙƒØ² Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ÙˆØ§Ù„ÙØ±Øµ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØ© Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ©. Ø§Ø³ØªØ®Ø¯Ù… Ù„ØºØ© ÙˆØ§Ø¶Ø­Ø© ÙˆÙ…ÙÙ‡ÙˆÙ…Ø© ÙˆØ¨Ø£Ø³Ù„ÙˆØ¨ ÙŠÙ†Ø§Ø³Ø¨ Ø¬ÙŠÙ„ Z - Ø®Ù„ÙŠÙ‡Ø§ ÙƒÙˆÙ„ ÙˆØ¹ÙÙˆÙŠØ© ÙˆÙ…Ø¨Ø§Ø´Ø±Ø© ğŸ”¥"

    try:
        logger.info(f"Generating description for: {request.title} (Uni: {request.university_name})")
        # Using the newer OpenAI client syntax for chat completions
        client = openai.OpenAI(api_key=openai.api_key) # Create client instance
        completion = client.chat.completions.create(
            model="gpt-3.5-turbo", # Or use a more advanced model if preferred
            messages=[
                {"role": "system", "content": "Ø§Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø¨ØªØ³Ø§Ø¹Ø¯ Ø§Ù„Ø·Ù„Ø§Ø¨ ÙŠÙÙ‡Ù…ÙˆØ§ ØªØ®ØµØµØ§Øª Ø§Ù„Ø¬Ø§Ù…Ø¹Ø©. Ø§Ø³ØªØ®Ø¯Ù… Ù„ØºØ© Ø´Ø¨Ø§Ø¨ÙŠØ© Ø¹ØµØ±ÙŠØ© ÙˆÙ…Ø¨Ø§Ø´Ø±Ø© ğŸ”¥"},
                {"role": "user", "content": prompt}
            ],
            max_tokens=100, # Limit response length
            temperature=0.7, # Balance creativity and focus
            n=1,
            stop=None,
        )
        
        # Ensure choices are available and contain message content
        if completion.choices and completion.choices[0].message and completion.choices[0].message.content:
            description = completion.choices[0].message.content.strip()
            logger.info(f"Generated description: {description}")
            return GenerateDescriptionResponse(description=description)
        else:
             logger.error("OpenAI response was empty or malformed.")
             raise HTTPException(status_code=500, detail="Failed to generate description: Empty response from AI.")

    except openai.AuthenticationError:
        logger.error("OpenAI Authentication Error: Check API key.")
        raise HTTPException(status_code=500, detail="AI service authentication failed.")
    except openai.RateLimitError:
        logger.warning("OpenAI Rate Limit Exceeded.")
        raise HTTPException(status_code=429, detail="Rate limit exceeded for AI service. Please try again later.")
    except Exception as e:
        logger.error(f"Error generating description: {e}")
        raise HTTPException(status_code=500, detail=f"An error occurred while generating the description: {e}")

# --- Generate Comparison Table Function ---
def generate_comparison_table_data(major_name: str, info_type: str, all_university_ids: List[str], current_university_id: str) -> str:
    """Generates a Markdown table comparing a major's info across universities."""
    logger.info(f"Generating comparison for Major: '{major_name}', Info: '{info_type}' across {len(all_university_ids)} unis.")

    if not majors_data:
        logger.error("Majors data not loaded, cannot generate comparison.")
        return "Ø§Ø¹ØªØ°Ø±ØŒ Ù„Ø§ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø­Ø§Ù„ÙŠÙ‹Ø§ Ø¨Ø³Ø¨Ø¨ Ø¹Ø¯Ù… ØªÙˆÙØ± Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ®ØµØµØ§Øª."

    headers = ["Ø§Ù„Ø¬Ø§Ù…Ø¹Ø©", info_type, "Ù…Ù„Ø§Ø­Ø¸Ø§Øª"]
    rows = []

    # Get full university names for display
    university_display_names = {
        "aaup": "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø£Ù…Ø±ÙŠÙƒÙŠØ©",
        "birzeit": "Ø¨ÙŠØ±Ø²ÙŠØª",
        "ppu": "Ø¨ÙˆÙ„ÙŠØªÙƒÙ†Ùƒ ÙÙ„Ø³Ø·ÙŠÙ†",
        "an-najah": "Ø¬Ø§Ù…Ø¹Ø© Ø§Ù„Ù†Ø¬Ø§Ø­ Ø§Ù„ÙˆØ·Ù†ÙŠØ©",
        "bethlehem": "Ø¨ÙŠØª Ù„Ø­Ù…",
        "alquds": "Ø¬Ø§Ù…Ø¹Ø© Ø§Ù„Ù‚Ø¯Ø³"
    }

    for uni_id in all_university_ids:
        uni_display_name = university_display_names.get(uni_id, uni_id)
        found_major_at_uni = False
        info_value = "ØºÙŠØ± Ù…ØªÙˆÙØ±"
        notes = ""

        for major_dict in majors_data:
            if major_dict.get('university', '').lower() == uni_id.lower():
                # Simple title match (case-insensitive, substring) - can be improved
                # Normalize titles slightly by removing common prefixes/suffixes if needed
                major_title_in_data = major_dict.get('title', '').lower().strip()
                query_major_name_lower = major_name.lower().strip()
                
                # Example: If major_name is "Computer Science" and title in data is "BSc Computer Science"
                if query_major_name_lower in major_title_in_data or major_title_in_data in query_major_name_lower:
                    try:
                        parsed_major = parse_major_details(major_dict.copy()) # Use a copy to avoid modifying original
                        found_major_at_uni = True

                        if "Ø±Ø³ÙˆÙ…" in info_type: # Check for fee
                            if parsed_major.parsed_fee is not None:
                                currency_str = f" {parsed_major.parsed_currency}" if parsed_major.parsed_currency else ""
                                info_value = f"{parsed_major.parsed_fee}{currency_str}"
                            else:
                                info_value = "Ù„Ù… ÙŠØªÙ… ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø±Ø³ÙˆÙ…"
                        elif "Ù‚Ø¨ÙˆÙ„" in info_type: # Check for admission average
                            if parsed_major.parsed_min_avg is not None:
                                info_value = f"{parsed_major.parsed_min_avg}%"
                                if parsed_major.parsed_branches:
                                    notes = f"Ø§Ù„Ø£ÙØ±Ø¹: {', '.join(parsed_major.parsed_branches)}"
                                else:
                                    notes = "Ù„Ù… ØªØ­Ø¯Ø¯ Ø§Ù„Ø£ÙØ±Ø¹"
                            else:
                                info_value = "Ù„Ù… ÙŠØ­Ø¯Ø¯ Ø§Ù„Ù…Ø¹Ø¯Ù„"
                        
                        # Highlight current university
                        if uni_id == current_university_id:
                             uni_display_name = f"ğŸ“ {uni_display_name} (Ø§Ù„Ø­Ø§Ù„ÙŠØ©)"
                        break # Found major for this uni, move to next uni
                    except Exception as e:
                        logger.warning(f"Error parsing major {major_dict.get('id')} for {uni_id} during comparison: {e}")
                        info_value = "Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©"
                        break
        
        if not found_major_at_uni:
            notes = f"Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ØªØ®ØµØµ '{major_name}' Ø¨Ù‡Ø°Ù‡ Ø§Ù„Ø¬Ø§Ù…Ø¹Ø© Ø£Ùˆ ØªÙØ§ØµÙŠÙ„Ù‡ ØºÙŠØ± Ù…ØªØ§Ø­Ø©."
            if uni_id == current_university_id:
                 uni_display_name = f"ğŸ“ {uni_display_name} (Ø§Ù„Ø­Ø§Ù„ÙŠØ©)"

        rows.append([uni_display_name, info_value, notes])

    # Construct Markdown table
    table = f"**Ù…Ù‚Ø§Ø±Ù†Ø© {info_type} Ù„ØªØ®ØµØµ \"{major_name}\" Ø¹Ø¨Ø± Ø§Ù„Ø¬Ø§Ù…Ø¹Ø§Øª:**\n\n"
    table += "| " + " | ".join(headers) + " |\n"
    table += "| " + " | ".join(["---" for _ in headers]) + " |\n"
    for row_data in rows:
        table += "| " + " | ".join(str(item) for item in row_data) + " |\n"
    
    table += "\n*Ù…Ù„Ø§Ø­Ø¸Ø©: Ù‡Ø°Ù‡ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù‡ÙŠ Ù„Ø£ØºØ±Ø§Ø¶ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© ÙˆÙ‚Ø¯ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ£ÙƒÙŠØ¯ Ù…Ù† Ø§Ù„Ø¬Ø§Ù…Ø¹Ø© Ù…Ø¨Ø§Ø´Ø±Ø©.*"
    logger.info(f"Generated comparison table:\n{table}")
    return table
# --- End Generate Comparison Table Function ---

if __name__ == "__main__":
    import uvicorn
    uvicorn.run("app:app", host="0.0.0.0", port=8000, reload=True) 
